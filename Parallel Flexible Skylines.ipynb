{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c3b2bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import time\n",
    "import cdd\n",
    "import enum\n",
    "from math import sqrt, atan, floor, ceil, pi\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "# libraries for LP \n",
    "from pulp import LpMinimize, LpProblem, LpStatus, LpVariable, LpMaximize, PULP_CBC_CMD\n",
    "import gurobipy as gp\n",
    "\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "conf=pyspark.SparkConf().setMaster(\"local[20]\").setAppName(\"Skyline Algos\")\n",
    "#conf=pyspark.SparkConf().setMaster('spark://master:7077').setSparkHome('/usr/local/spark').setAppName(\"Skyline Algos\")\n",
    "\n",
    "#conf.set(\"spark.executor.instances\", \"1\")\n",
    "#conf.set(\"spark.executor.memory\", \"6g\")\n",
    "#conf.set(\"spark.worker.memory\", \"18g\")\n",
    "#conf.set(\"spark.driver.bindAddress\", \"10.75.4.81\")\n",
    "#conf.set(\"spark.driver.port\", \"8619\")\n",
    "#conf.set(\"spark.driver.host\", \"10.75.4.81\")\n",
    "conf.set(\"spark.driver.memory\", \"2g\")\n",
    "#conf.set(\"spark.executor.memory\", \"4g\")\n",
    "\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7035c8ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#global value for the coefficient of the Constraints\n",
    "#case 2 dimension\n",
    "# -x + y <= 0\n",
    "# 0 <= x <= 1\n",
    "# 0 <= y <= 1\n",
    "A = np.array([[-1, 1],\n",
    "                 [-1, 0],\n",
    "                 [1, 0],\n",
    "                 [0, -1],\n",
    "                 [0, 1]])\n",
    "b = np.array([[0],[0],[1],[0],[1]])\n",
    "# x + y = 1\n",
    "C = np.array([[1, 1]])\n",
    "d = np.array([[1]])\n",
    "\n",
    "#case 4 dimensions\n",
    "# -x + y <= 0\n",
    "# 0 <= x <= 1\n",
    "# 0 <= y <= 1\n",
    "# 0 <= z <= 1\n",
    "# 0 <= u <= 1\n",
    "E = np.array([[-1, 1, 0, 0],\n",
    "                 [-1, 0, 0, 0],\n",
    "                 [1, 0, 0, 0],\n",
    "                 [0, -1, 0, 0],\n",
    "                 [0, 1, 0, 0],\n",
    "                 [0, 0, -1, 0],\n",
    "                 [0, 0, 1, 0],\n",
    "                 [0, 0, 0, -1],\n",
    "                 [0, 0, 0, 1]])\n",
    "f = np.array([[0],[0],[1],[0],[1],[0],[1],[0],[1]])\n",
    "# x + y +z + u= 1\n",
    "G = np.array([[1, 1, 1, 1]])\n",
    "h = np.array([[1]])\n",
    "\n",
    "#case 6 dimensions\n",
    "# -x + y <= 0\n",
    "# 0 <= x <= 1\n",
    "# 0 <= y <= 1\n",
    "# 0 <= z <= 1\n",
    "# 0 <= u <= 1\n",
    "#...\n",
    "I = np.array([[-1, 1, 0, 0, 0, 0],\n",
    "                 [-1, 0, 0, 0,0,0],\n",
    "                 [1, 0, 0, 0,0,0],\n",
    "                 [0, -1, 0, 0,0,0],\n",
    "                 [0, 1, 0, 0,0,0],\n",
    "                 [0, 0, -1, 0,0,0],\n",
    "                 [0, 0, 1, 0,0,0],\n",
    "                 [0, 0, 0, -1,0,0],\n",
    "                 [0, 0, 0, 1,0,0],\n",
    "                 [0, 0, 0, 0,-1,0],\n",
    "                 [0, 0, 0, 0,1,0],\n",
    "                 [0, 0, 0, 0,0,-1],\n",
    "                 [0, 0, 0, 0,0,1]])\n",
    "l = np.array([[0],[0],[1],[0],[1],[0],[1],[0],[1],[0],[1],[0],[1]])\n",
    "# x + y +z + u + ..= 1\n",
    "M = np.array([[1, 1, 1, 1, 1, 1]])\n",
    "n = np.array([[1]])\n",
    "#case 7 dimensions\n",
    "# -x + y <= 0\n",
    "# 0 <= x <= 1\n",
    "# 0 <= y <= 1\n",
    "# 0 <= z <= 1\n",
    "# 0 <= u <= 1\n",
    "#...\n",
    "O = np.array([[-1, 1, 0, 0, 0, 0, 0],\n",
    "                 [-1,0, 0, 0, 0, 0, 0],\n",
    "                 [1, 0, 0, 0, 0, 0, 0],\n",
    "                 [0,-1, 0, 0, 0, 0, 0],\n",
    "                 [0, 1, 0, 0, 0, 0, 0],\n",
    "                 [0, 0,-1, 0, 0, 0, 0],\n",
    "                 [0, 0, 1, 0, 0, 0, 0],\n",
    "                 [0, 0, 0,-1, 0, 0, 0],\n",
    "                 [0, 0, 0, 1, 0, 0, 0],\n",
    "                 [0, 0, 0, 0,-1, 0, 0],\n",
    "                 [0, 0, 0, 0, 1, 0, 0],\n",
    "                 [0, 0, 0, 0, 0,-1, 0],\n",
    "                 [0, 0, 0, 0, 0, 1, 0],\n",
    "                 [0, 0, 0, 0, 0, 0, -1],\n",
    "                 [0, 0, 0, 0, 0, 0, 1],])\n",
    "p_ = np.array([[0],[0],[1],[0],[1],[0],[1],[0],[1],[0],[1],[0],[1],[0],[1]])\n",
    "# x + y +z + u + ..= 1\n",
    "Q = np.array([[1, 1, 1, 1, 1, 1, 1]])\n",
    "r = np.array([[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "721d2a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to check if a tuple dominates another tuple\n",
    "def dominates(a, b):\n",
    "    hasStrict = False\n",
    "    \n",
    "    for i in range(len(a)):\n",
    "        if a[i] > b[i]:\n",
    "            return False\n",
    "        elif a[i] < b[i]:\n",
    "            hasStrict = True\n",
    "    return hasStrict\n",
    "\n",
    "#function to compute the value of one side of the inequality in VE version\n",
    "def computeInequality(tupl, vertices, p):\n",
    "    res = []\n",
    "    for i in range(len(vertices)):\n",
    "        value = 0\n",
    "        for j in range(len(tupl)):\n",
    "            value += vertices[i][j]*(tupl[j]**p)\n",
    "        res.append(value)   \n",
    "    #print(res)\n",
    "    return res\n",
    "\n",
    "#function that computes the objective function of the LP based on the value of the two tuples that we are comparing\n",
    "def objective_function(i, j, p):\n",
    "    obj_func_w = []\n",
    "    for k in range(len(i)):\n",
    "        obj_func_w.append(i[k]**p - j[k]**p)\n",
    "    \n",
    "    return obj_func_w\n",
    "\n",
    "#function that computes the coordinates of the centroid using the vertex of the polyhedron\n",
    "def compute_centroid(vertices):\n",
    "    sum_axis = []\n",
    "    centroid = []\n",
    "    for i in range(len(vertices[0])):\n",
    "        sum_axis.append(0)\n",
    "        centroid.append(0)\n",
    "    for i in range(len(vertices)):\n",
    "        for j in range(len(vertices[i])):\n",
    "            sum_axis[j] += vertices[i][j]\n",
    "    for i in range(len(sum_axis)):\n",
    "        centroid[i] = sum_axis[i] / len(vertices)\n",
    "    \n",
    "    return centroid\n",
    "\n",
    "#sort function based on weights (e.g. centroid coordinates)\n",
    "def sort_function(data, weight):\n",
    "    value = 0\n",
    "    for i in range(len(data)):\n",
    "        value += data[i]*weight[i]\n",
    "    return value\n",
    "\n",
    "#function to compute the vertices from a set of equation\n",
    "def computeVertices(A,b,C,d):\n",
    "    m_ineq = np.hstack( (b, -A) )\n",
    "\n",
    "    mat = cdd.Matrix(m_ineq, number_type='float') \n",
    "    mat.rep_type = cdd.RepType.INEQUALITY\n",
    "\n",
    "    m_eq = np.hstack( (d, -C) )\n",
    "    mat.extend(m_eq, linear=True)\n",
    "    #print(mat.__getitem__(0), mat.__getitem__(1))\n",
    "    # print(mat.lin_set)\n",
    "\n",
    "    poly = cdd.Polyhedron(mat)\n",
    "    ext = poly.get_generators()\n",
    "    #print(ext)\n",
    "    vertices = []\n",
    "    for i in range(len(ext)):\n",
    "        w2 = []\n",
    "        for j in range(len(ext[i][1:])):\n",
    "            w2.append(ext[i][j+1])\n",
    "        vertices.append(w2)\n",
    "    print('Vertices: ' + str(vertices))\n",
    "    \n",
    "    return vertices\n",
    "\n",
    "#function that computes the left side of the equations in the po algorithm\n",
    "def computeLeftSide(variables,po,i,p):\n",
    "    value= 0 \n",
    "    for j in range(len(variables)):\n",
    "        value += variables[j]*(po[j][i]**p)\n",
    "    return value\n",
    "\n",
    "def getVariables(numDim):\n",
    "    variables = []\n",
    "    for i in range(numDim):\n",
    "        variables.append(LpVariable(name= \"x\" + str(i), lowBound= 0,upBound=1))\n",
    "    \n",
    "    return variables\n",
    "\n",
    "def getVariables_primal(numDim):\n",
    "    variables = []\n",
    "    variables.append(LpVariable(name= \"phi\"))\n",
    "    for i in range(numDim):\n",
    "        variables.append(LpVariable(name= \"x\" + str(i), lowBound= 0,upBound=1))\n",
    "\n",
    "    return variables\n",
    "\n",
    "#function that creates a gurobipy model for the dual computation of po\n",
    "def getModel_dual(num):\n",
    "    with gp.Env(empty=True) as env:\n",
    "        env.setParam('OutputFlag', 0)\n",
    "        env.start()\n",
    "        m=gp.Model(env=env)\n",
    "    m.Params.LogToConsole = 0\n",
    "    variables = []\n",
    "    for i in range(num):\n",
    "        x= m.addVar(lb=0.0, ub=1.0, vtype=gp.GRB.CONTINUOUS, name=\"x\" + str(i))\n",
    "        variables.append(x)\n",
    "    \n",
    "    return m,variables\n",
    "\n",
    "#function that creates a gurobipy model for the computation of nd\n",
    "def getModel_nd(num):\n",
    "    with gp.Env(empty=True) as env:\n",
    "        env.setParam('OutputFlag', 0)\n",
    "        env.start()\n",
    "        m=gp.Model(env=env)\n",
    "    m.Params.LogToConsole = 0\n",
    "    variables = []\n",
    "    var_names = []\n",
    "    for i in range(num):\n",
    "        x= m.addVar(lb=0.0, ub=1.0, vtype=gp.GRB.CONTINUOUS, name=\"x\" + str(i))\n",
    "        variables.append(x)\n",
    "        var_names.append(\"x\"+str(i))\n",
    "    \n",
    "    return m,variables,var_names\n",
    "\n",
    "#function that creates a gurobipy model for the primal computation of po\n",
    "def getModel_primal(numDim):\n",
    "    with gp.Env(empty=True) as env:\n",
    "        env.setParam('OutputFlag', 0)\n",
    "        env.start()\n",
    "        m=gp.Model(env=env)\n",
    "    m.Params.LogToConsole = 0\n",
    "    variables = []\n",
    "    var_names = []\n",
    "    phi= m.addVar(vtype=gp.GRB.CONTINUOUS, name=\"phi\")\n",
    "    variables.append(phi)\n",
    "    var_names.append(\"phi\")\n",
    "    for i in range(numDim):\n",
    "        x= m.addVar(lb=0.0, ub=1.0, vtype=gp.GRB.CONTINUOUS, name=\"w\" + str(i))\n",
    "        variables.append(x)\n",
    "        var_names.append(\"w\"+str(i))\n",
    "    \n",
    "    return m,variables,var_names\n",
    "\n",
    "#function that compute the volume \n",
    "def volume(p):\n",
    "    volume = 1\n",
    "    for i in range(len(p)):\n",
    "        volume *= p[i]\n",
    "    return volume\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add922a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that find the skyline using BNL\n",
    "def find_skyline_bnl(data):\n",
    "    #Finds the skyline using a block-nested loop.\n",
    "    if not isinstance(data,list):\n",
    "        data = list(data)\n",
    "    skyline = []\n",
    "    c = 0\n",
    "    for i in data:\n",
    "        is_dominated = False\n",
    "        for j in skyline:\n",
    "            c += 1\n",
    "            if dominates(j,i):\n",
    "                is_dominated = True\n",
    "                break\n",
    "        if is_dominated:\n",
    "            continue\n",
    "        # removing dominated points from the window\n",
    "        to_drop = []\n",
    "        for k in skyline:\n",
    "            c += 1\n",
    "            if dominates(i, k):\n",
    "                to_drop.append(k)\n",
    "\n",
    "        for drop in to_drop:\n",
    "            skyline.remove(drop)\n",
    "\n",
    "        skyline.append(i)\n",
    "\n",
    "    print('comparisons:'+str(c))\n",
    "        \n",
    "    return skyline\n",
    "\n",
    "def find_skyline_sfs(data, weights):\n",
    "\n",
    "    if not isinstance(data,list):\n",
    "        data = list(data)\n",
    "\n",
    "    #sort the dataset using a sort function\n",
    "    data.sort(key = lambda x: sort_function(x, weights))\n",
    "\n",
    "    skyline = []\n",
    "    #c = 0\n",
    "    # Loop through the rest of the rows\n",
    "    for i in data:\n",
    "        is_dominated = False\n",
    "\n",
    "        for j in skyline:\n",
    "            #c+=1\n",
    "            if dominates(j,i):\n",
    "                is_dominated = True\n",
    "                break\n",
    "\n",
    "        if is_dominated:\n",
    "            continue\n",
    "\n",
    "        skyline.append(i)\n",
    "\n",
    "    #print('comparisons:'+str(c))\n",
    "    return skyline\n",
    "\n",
    "def find_skyline_sfs2(data):\n",
    "\n",
    "    if not isinstance(data,list):\n",
    "        data = list(data)\n",
    "\n",
    "    #sort the dataset using a sort function\n",
    "    data.sort()\n",
    "\n",
    "    skyline = []\n",
    "    c = 0\n",
    "    # Loop through the rest of the rows\n",
    "    for i in data:\n",
    "        is_dominated = False\n",
    "\n",
    "        for j in skyline:\n",
    "            c+=1\n",
    "            if dominates(j,i):\n",
    "                is_dominated = True\n",
    "                break\n",
    "\n",
    "        if is_dominated:\n",
    "            continue\n",
    "\n",
    "        skyline.append(i)\n",
    "\n",
    "    print('comparisons:'+str(c))\n",
    "    return skyline\n",
    "\n",
    "\n",
    "\n",
    "#function that finds the skyline using SaLSa\n",
    "def find_skyline_SaLSa(data):\n",
    "    if not isinstance(data,list):\n",
    "        data = list(data)\n",
    "    c = 0\n",
    "    if len(data)== 0:\n",
    "        return []\n",
    "    data.sort(key= lambda x: (min(x), sum(x)))\n",
    "    \n",
    "    skyline = []\n",
    "    \n",
    "    p_stop = data[0]\n",
    "    \n",
    "    for p in data:\n",
    "        is_dominated = False\n",
    "        if (min(p) > max(p_stop)) or ((min(p) == max(p_stop)) and p != p_stop):\n",
    "            break\n",
    "      \n",
    "        for i in skyline:\n",
    "            c += 1\n",
    "            if dominates(i, p):\n",
    "                is_dominated = True\n",
    "                break\n",
    "        \n",
    "        if not is_dominated:\n",
    "            if max(p) < max(p_stop):\n",
    "                p_stop = p\n",
    "            \n",
    "            skyline.append(p)\n",
    "    print('comparisons: '+str(c))\n",
    "    return skyline\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1797304",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sfs_multithread(datapoints, global_set, weights):\n",
    "    nd = []\n",
    "    \n",
    "    datapoints = list(datapoints)\n",
    "        \n",
    "    datapoints.sort(key=lambda x: sort_function(x, weights))\n",
    "    \n",
    "    for ps in datapoints:\n",
    "        # ps : potentially non dominated point\n",
    "        for other in global_set:\n",
    "            if ps == other:\n",
    "                nd.append(ps)\n",
    "                break\n",
    "            dominated = False\n",
    "            if dominates(other, ps):\n",
    "                dominated = True\n",
    "                break\n",
    "            \n",
    "#         for other in global_set:\n",
    "#             # dominated other\n",
    "#             dominated = False\n",
    "#             # num of dimensions in which the points are equal\n",
    "#             dimEqual = 0\n",
    "#             for k in range(len(global_set[0])):\n",
    "#                 if ps[k] < other[k] :\n",
    "#                     dominated = True\n",
    "#                     break\n",
    "#                 elif other[k] == ps[k]:\n",
    "#                     dimEqual = dimEqual + 1\n",
    "#             if dominated == True:\n",
    "#                 continue\n",
    "#             # We suppose that the global_set is ordered. \n",
    "#             # Keeping in mind that the global_set is a superset of our datapoints, if we find our point in the global_set\n",
    "#             # then all other points can not dominated this current point.\n",
    "#             if dimEqual == len(global_set[0]):\n",
    "#                 nd.append(ps)\n",
    "#                 break\n",
    "            \n",
    "#             break\n",
    "            \n",
    "    return nd\n",
    "\n",
    "\n",
    "def salsa_multithread(data, global_set):\n",
    "    nd = []\n",
    "\n",
    "    if not isinstance(data,list):\n",
    "        data = list(data)\n",
    "\n",
    "    if len(data)== 0:\n",
    "        return []\n",
    "    data.sort(key= lambda x: (min(x), sum(x)))\n",
    "\n",
    "    p_stop = data[0]\n",
    "\n",
    "    for p in data:\n",
    "        # ps : potentially non dominated point\n",
    "        if (min(p) > max(p_stop)) or ((min(p) == max(p_stop)) and p != p_stop):\n",
    "            break\n",
    "        for other in global_set:\n",
    "            # dominated other\n",
    "            dominated = False\n",
    "            # num of dimensions in which the points are equal\n",
    "            dimEqual = 0\n",
    "            for k in range(len(global_set[0])):\n",
    "                if p[k] < other[k] :\n",
    "                    dominated = True\n",
    "                    break\n",
    "                elif other[k] == p[k]:\n",
    "                    dimEqual = dimEqual + 1\n",
    "            if dominated == True:\n",
    "                continue\n",
    "\n",
    "            if max(p) < max(p_stop):\n",
    "                p_stop = p\n",
    "            # We suppose that the global_set is ordered.\n",
    "            # Keeping in mind that the global_set is a superset of our datapoints, if we find our point in the global_set\n",
    "            # then all other points can not dominated this current point.\n",
    "            if dimEqual == len(global_set[0]):\n",
    "                nd.append(p)\n",
    "                break\n",
    "\n",
    "            break\n",
    "\n",
    "    return nd\n",
    "\n",
    "def sve1f_multithread(data, global_set, vertices, p):\n",
    "\n",
    "    if not isinstance(data,list):\n",
    "        data = list(data)\n",
    "\n",
    "    if len(data) == 0:\n",
    "        return []\n",
    "\n",
    "    centroids =  compute_centroid(vertices)\n",
    "    #print(centr)\n",
    "    data.sort(key=lambda x: sort_function(x, centroids))\n",
    "    nd = []\n",
    "    for d in data:\n",
    "        left_hand_side = computeInequality(d, vertices, p)\n",
    "        is_dominated = True\n",
    "        for other in global_set:\n",
    "            # dominated other\n",
    "            if other == d:\n",
    "                nd.append(d)\n",
    "                break\n",
    "            if dominates(other, d):\n",
    "                break\n",
    "            right_hand_side = computeInequality(other, vertices, p)\n",
    "            for k in range(len(right_hand_side)): #if s satisfies all the inequalities, t F-dominates s otherwise no\n",
    "                is_dominated = True\n",
    "                if left_hand_side[k] < right_hand_side[k]:\n",
    "                    is_dominated = False\n",
    "                    break\n",
    "            if is_dominated:\n",
    "                break\n",
    "\n",
    "    return nd\n",
    "\n",
    "def pond_primal_pulp_multithread(data, global_set, constraints, k_):\n",
    "    if not isinstance(data,list):\n",
    "        data = list(data)\n",
    "\n",
    "    if len(data) == 0:\n",
    "        return []\n",
    "\n",
    "    delta = 2\n",
    "    lastRound = False\n",
    "    minimum = 0\n",
    "    dimensions = len(data[0])\n",
    "\n",
    "    while(not lastRound):\n",
    "\n",
    "        if delta >= (len(global_set) - 1):\n",
    "            lastRound = True\n",
    "        data_reversed = data.copy()\n",
    "        data_reversed.reverse()\n",
    "        for t in data_reversed: #candidate F-dominated tuple\n",
    "            model = LpProblem(sense=LpMaximize)\n",
    "            #Prepare the LP model\n",
    "            variables = getVariables_primal(dimensions)\n",
    "\n",
    "            temp2 = 0\n",
    "            for cons in constraints:\n",
    "                i = 0\n",
    "                for index in range(len(cons)):\n",
    "                    temp2 += cons[index]*variables[index+1]\n",
    "                model += (temp2 <= k_[i])\n",
    "                i += 1\n",
    "\n",
    "            minimum = min(delta, len(global_set)-1)\n",
    "            if minimum == 0:\n",
    "                break\n",
    "            temp = 0\n",
    "            for i in range(len(variables)-1):\n",
    "                temp += variables[i+1]\n",
    "            model += temp == 1\n",
    "\n",
    "            po_temp = global_set.copy()\n",
    "            po_temp.remove(t)\n",
    "\n",
    "            for j in range(minimum): #compute the inequalities and add it to the LP model\n",
    "                left_side = 0\n",
    "                for i in range(dimensions):\n",
    "                    left_side += variables[i+1] * (t[i]-po_temp[j][i])\n",
    "                model += (left_side + variables[0] <= 0)\n",
    "            model.setObjective(variables[0])\n",
    "\n",
    "            model.solve(PULP_CBC_CMD(msg=False))\n",
    "\n",
    "            if (model.objective.value() > 0):\n",
    "                continue\n",
    "\n",
    "            data.remove(t)\n",
    "            global_set.remove(t)\n",
    "\n",
    "            del model\n",
    "\n",
    "        delta = delta * 2\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def po_primal_pulp_non_inc_multi(po, global_set, constraints, k_):\n",
    "    if not isinstance(po,list):\n",
    "        po = list(po)\n",
    "\n",
    "    if len(po) == 0:\n",
    "        return []\n",
    "    dimensions = len(po[0])\n",
    "    po_reversed = po.copy()\n",
    "    po_reversed.reverse()\n",
    "    for t in po_reversed:\n",
    "        model = LpProblem(sense=LpMaximize)\n",
    "        #Prepare the LP model\n",
    "        variables = getVariables_primal(dimensions)\n",
    "        temp2 = 0\n",
    "        for cons in constraints:\n",
    "            i = 0\n",
    "            for index in range(len(cons)):\n",
    "                temp2 += cons[index]*variables[index+1]\n",
    "            model += (temp2 <= k_[i])\n",
    "            i += 1\n",
    "        temp = 0\n",
    "        for i in range(len(variables)-1):\n",
    "            temp += variables[i+1]\n",
    "\n",
    "        model += temp == 1\n",
    "        po_temp = global_set.copy()\n",
    "        po_temp.remove(t)\n",
    "        for j in range(len(po_temp)): #compute the inequalities and add it to the LP model\n",
    "            left_side = 0\n",
    "            for i in range(dimensions):\n",
    "                left_side += variables[i+1] * (t[i]-po_temp[j][i])\n",
    "            model += (left_side + variables[0] <= 0)\n",
    "        model.setObjective(variables[0])\n",
    "        model.solve(PULP_CBC_CMD(msg=False))\n",
    "\n",
    "        if (model.objective.value() > 0): # PO contains t iff the linear system is infeasible, otherwise is F-dominated by a convex combination of the other\n",
    "#                 print(\"Removed \"+ str(t))\n",
    "            continue\n",
    "            #po_reversed.remove(t)\n",
    "        po.remove(t)\n",
    "        del model\n",
    "\n",
    "    return po\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "375a13a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#unpack each data by it's first index\n",
    "def execute_sfs_indexed(input_list, weights):\n",
    "    i = 0\n",
    "    nd = []\n",
    "    for el_list in input_list:\n",
    "        nd.append(el_list[1])\n",
    "    nd = find_skyline_sfs(nd,weights)\n",
    "    return nd\n",
    "\n",
    "def execute_sfs_indexed2(input_list, weights):\n",
    "    input_list = []\n",
    "    for l in f:\n",
    "        p =l.strip().split(\" \")\n",
    "\n",
    "        point=[]\n",
    "        for x in p:\n",
    "            point.append(float(x))\n",
    "        input_list.append(point)\n",
    "    i = 0\n",
    "    nd = []\n",
    "    for el_list in input_list:\n",
    "        nd.append(el_list[1])\n",
    "    nd = find_skyline_sfs(nd,weights)\n",
    "    return nd\n",
    "\n",
    "\n",
    "def execute_sfs_indexed_with_memory(input_list, memory_list,weights):\n",
    "    i = 0\n",
    "    nd = []\n",
    "    for el_list in input_list:\n",
    "        nd.append(el_list[1])\n",
    "    nd = sfs_with_memory(nd, memory_list, weights)\n",
    "    return nd\n",
    "\n",
    "def execute_saLSa_indexed(input_list):\n",
    "    i = 0\n",
    "    nd = []\n",
    "    for el_list in input_list:\n",
    "        nd.append(el_list[1])\n",
    "    nd = find_skyline_SaLSa(nd)\n",
    "    return nd\n",
    "\n",
    "def execute_salsa_indexed_with_memory(input_list, memory_list):\n",
    "    i = 0\n",
    "    nd = []\n",
    "    for el_list in input_list:\n",
    "        nd.append(el_list[1])\n",
    "    nd = salsa_with_memory(nd, memory_list)\n",
    "    return nd\n",
    "def execute_sve1_filter_with_memory(input_list, representative, vertices, p, onlyFilter):\n",
    "    i = 0\n",
    "    nd = []\n",
    "    for el_list in input_list:\n",
    "        nd.append(el_list[1])\n",
    "    nd = filter_nd_with_memory2(nd, representative, vertices, p, onlyFilter)\n",
    "    return nd\n",
    "\n",
    "def execute_sve1_indexed(input_list, vertices, p):\n",
    "    i = 0\n",
    "    nd = []\n",
    "    for el_list in input_list:\n",
    "        nd.append(el_list[1])\n",
    "    nd = sve1(nd,vertices, p)\n",
    "    return nd\n",
    "\n",
    "def execute_sve1_indexed_with_memory(input_list, memory_list, vertices, p):\n",
    "    i = 0\n",
    "    nd = []\n",
    "    for el_list in input_list:\n",
    "        nd.append(el_list[1])\n",
    "    nd = sve1_with_memory(nd, memory_list, vertices, p)\n",
    "    return nd\n",
    "\n",
    "def execute_sve1f_indexed(input_list, vertices, p):\n",
    "    i = 0\n",
    "    nd = []\n",
    "    for el_list in input_list:\n",
    "        nd.append(el_list[1])\n",
    "    nd = sve1f(nd,vertices, p)\n",
    "    return nd\n",
    "def execute_sve1f_multithread(input_list, global_set, vertices, p):\n",
    "    i = 0\n",
    "    nd = []\n",
    "    for el_list in input_list:\n",
    "        nd.append(el_list[1])\n",
    "    nd = sve1f_multithread(nd,global_set, vertices, p)\n",
    "    return nd\n",
    "\n",
    "def execute_sve1f_indexed_with_memory(input_list, memory_list, vertices, p):\n",
    "    i = 0\n",
    "    nd = []\n",
    "    for el_list in input_list:\n",
    "        nd.append(el_list[1])\n",
    "    nd = sve1f_with_memory(nd, memory_list, vertices, p)\n",
    "    return nd\n",
    "\n",
    "def execute_POND_indexed(input_list, vertices, p):\n",
    "    i = 0\n",
    "    nd = []\n",
    "    for el_list in input_list:\n",
    "        nd.append(el_list[1])\n",
    "    nd = POND(nd,vertices, p)\n",
    "    return nd\n",
    "\n",
    "def execute_pond_indexed_dual(input_list, vertices, p):\n",
    "    i = 0\n",
    "    nd = []\n",
    "    for el_list in input_list:\n",
    "        nd.append(el_list[1])\n",
    "    nd = po_dual(nd,vertices, p)\n",
    "    return nd\n",
    "\n",
    "def execute_pond_indexed_primal(input_list, constraints, k_):\n",
    "    i = 0\n",
    "    nd = []\n",
    "    for el_list in input_list:\n",
    "        nd.append(el_list[1])\n",
    "    nd = po_primal(nd, constraints, k_)\n",
    "    return nd\n",
    "\n",
    "\n",
    "def execute_pond_indexed_primal_pulp(input_list, constraints, k_):\n",
    "    i = 0\n",
    "    nd = []\n",
    "    for el_list in input_list:\n",
    "        nd.append(el_list[1])\n",
    "    nd = po_primal_pulp(nd, constraints, k_)\n",
    "    return nd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "365f43b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#function that normalize the data in the dataset\n",
    "def normalize_data(data):\n",
    "    if not isinstance(data, list):\n",
    "        data = list(data)\n",
    "    return ( 0.999999*(data - np.min(data))) / (np.max(data) - np.min(data))\n",
    "\n",
    "### Data generation configuration\n",
    "\n",
    "# Type of dataset generation\n",
    "class DataGenEnum(enum.Enum):\n",
    "    antiCorrelated = 1\n",
    "    anticorrelated = 1\n",
    "    Anticorrelated = 1\n",
    "    AntiCorrelated = 1\n",
    "    correlated = 2\n",
    "    Correlated = 2\n",
    "    Independent = 3\n",
    "    independent = 3\n",
    "    \n",
    "    \n",
    "class DataGenConfig():\n",
    "    def __init__(self, typeOfCorrelation = DataGenEnum.independent, \n",
    "                 dataRange = [0,1], avg = 0.5, skylinePercentage = 1,\n",
    "                 numberOfData = 10**6, numberOfDimensions = 4,\n",
    "                 spreadPercentage = 10): \n",
    "        self.typeOfCorrelation = typeOfCorrelation\n",
    "        self.dataRange = dataRange\n",
    "        # UNUSED Variable\n",
    "        self.avg = avg\n",
    "        self.skylinePercentage = skylinePercentage\n",
    "        self.numberOfData = numberOfData\n",
    "        self.numberOfDimensions = numberOfDimensions\n",
    "        self.spreadPercentage = spreadPercentage\n",
    "        \n",
    "    def setCorrelated(self):\n",
    "            self.typeOfCorrelation = DataGenEnum.correlated\n",
    "    def setAntiCorrelated(self):\n",
    "            self.typeOfCorrelation = DataGenEnum.antiCorrelated\n",
    "    def setIndependent(self):\n",
    "            self.typeOfCorrelation = DataGenEnum.independent \n",
    "            \n",
    "    def setNumberOfData(self, numData):\n",
    "        self.numberOfData = numData\n",
    "    \n",
    "# Method that  creates the different types of datasets based on the distribution   \n",
    "def dataGenerator(dataConfiguration = None):\n",
    "    if dataConfiguration == None :\n",
    "        dataConfiguration = DataGenConfig()\n",
    "        \n",
    "    typeOfCorrelation = dataConfiguration.typeOfCorrelation\n",
    "    dataRange = dataConfiguration.dataRange\n",
    "    avg = dataConfiguration.avg\n",
    "    skylinePercentage = dataConfiguration.skylinePercentage\n",
    "    numberOfData = dataConfiguration.numberOfData\n",
    "    numberOfDimensions = dataConfiguration.numberOfDimensions\n",
    "    spreadPercentage = dataConfiguration.spreadPercentage\n",
    "    \n",
    "    minDataValue = dataRange[0]\n",
    "    maxDataValue = dataRange[1]\n",
    "    data = []\n",
    "    if typeOfCorrelation == DataGenEnum.independent:\n",
    "        for i in range(numberOfData):\n",
    "            datum = []\n",
    "            for i in range(numberOfDimensions):\n",
    "                datum.append(random.random()*(maxDataValue-minDataValue)+minDataValue)\n",
    "            data.append(datum)\n",
    "    elif typeOfCorrelation == DataGenEnum.correlated:\n",
    "        for i in range(numberOfData):\n",
    "            datum = []\n",
    "            datum.append(random.random()*(maxDataValue-minDataValue)+minDataValue)\n",
    "            relatedValue = datum[0]\n",
    "            spread = spreadPercentage * 0.01\n",
    "            for i in range(1, numberOfDimensions):\n",
    "                datum.append(relatedValue + ((random.random()-0.5)*spread) )\n",
    "            data.append(datum)\n",
    "    else: #typeOfCorrelation = antiCorrelated\n",
    "        for i in range(numberOfData):\n",
    "            datum = []\n",
    "            datum.append(random.random()*(maxDataValue-minDataValue)+minDataValue)\n",
    "            relatedValue = maxDataValue-datum[0]\n",
    "            spread = spreadPercentage * 0.01\n",
    "            for i in range(1, numberOfDimensions):\n",
    "                datum.append(relatedValue + (relatedValue*(random.random()-0.5)*spread) )\n",
    "            data.append(datum)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5abbbac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CENTRALIZED VERSION\n",
    "#First ND algorithm: BNL with LP + 2 phases: ULP2\n",
    "\n",
    "def ulp2(data, p, model, variables):\n",
    "    #PHASE 1\n",
    "    start = time.time()\n",
    "    skyline = find_skyline_bnl(data)\n",
    "    #print(nd)\n",
    "    end = time.time() - start\n",
    "    print('Number of points in the skyline ' + str(len(skyline)))\n",
    "    print('Time taken ' + str(end))\n",
    "    #PHASE 2\n",
    "    model_debug=True\n",
    "    nd = skyline.copy()\n",
    "    nd.reverse()\n",
    "    #count = 0\n",
    "    for s in skyline:\n",
    "#         count += 1\n",
    "#         print(count)\n",
    "        for t in nd:\n",
    "            if s==t:\n",
    "                continue\n",
    "            obj_fun_w = []\n",
    "            obj_fun_w = objective_function(s, t, p)\n",
    "            model2 = model.copy()\n",
    "            temp= 0\n",
    "            for pos in range(len(variables)):\n",
    "                temp += obj_fun_w[pos] * variables[pos]\n",
    "            model2 += temp\n",
    "            model2.solve(PULP_CBC_CMD(msg=False))\n",
    "            if model_debug:\n",
    "                print(model2)\n",
    "                model_debug=False\n",
    "            #print(model2.objective.value())\n",
    "            \n",
    "            if model2.objective.value()>=0:\n",
    "                #print(model2.objective.value(), s)\n",
    "                nd.remove(s)\n",
    "                break\n",
    "                   \n",
    "    return nd    \n",
    "\n",
    "#Second ND algorithm: SFS with LP + 2 phases: SLP2\n",
    "def slp2(data, p, model, centr, variables):\n",
    "    #PHASE 1\n",
    "    start = time.time()\n",
    "    skyline = find_skyline_sfs(data, centr)\n",
    "    #print(nd)\n",
    "    end = time.time() - start\n",
    "    print('Number of points in the skyline ' + str(len(skyline)))\n",
    "    print('Time taken ' + str(end))\n",
    "    #PHASE 2\n",
    "    model_debug=True\n",
    "    nd = []\n",
    "    #count = 0\n",
    "    for s in skyline: #Candidate F-dominated tuples\n",
    "#         count += 1\n",
    "#         print(count)\n",
    "        is_dominated = False\n",
    "        for t in nd: #candidate F-dominant tuple\n",
    "            obj_fun_w = []\n",
    "            obj_fun_w = objective_function(s, t, p)\n",
    "            model2 = model.copy()\n",
    "            temp= 0\n",
    "            for pos in range(len(variables)):\n",
    "                temp += obj_fun_w[pos] * variables[pos]\n",
    "            model2 += temp\n",
    "            model2.solve(PULP_CBC_CMD(msg=False))\n",
    "            if model_debug:\n",
    "                print(model2)\n",
    "                model_debug=False\n",
    "            #print(model2.objective.value())\n",
    "            \n",
    "            if model2.objective.value()>=0:  #if the solution of the LP has non-negative solution t f-dominates s\n",
    "                is_dominated = True\n",
    "                break\n",
    "        if is_dominated:\n",
    "            continue\n",
    "        nd.append(s)\n",
    "    return nd\n",
    "\n",
    "def slp2(data, p, model, centr, variables):\n",
    "    #PHASE 1\n",
    "    start = time.time()\n",
    "    skyline = find_skyline_sfs(data, centr)\n",
    "    #print(nd)\n",
    "    end = time.time() - start\n",
    "    print('Number of points in the skyline ' + str(len(skyline)))\n",
    "    print('Time taken ' + str(end))\n",
    "    #PHASE 2\n",
    "    model_debug=True\n",
    "    nd = []\n",
    "    #count = 0\n",
    "    for s in skyline: #Candidate F-dominated tuples\n",
    "#         count += 1\n",
    "#         print(count)\n",
    "        is_dominated = False\n",
    "        for t in nd: #candidate F-dominant tuple\n",
    "            obj_fun_w = []\n",
    "            obj_fun_w = objective_function(s, t, p)\n",
    "            model2 = model.copy()\n",
    "            temp= 0\n",
    "            for pos in range(len(variables)):\n",
    "                temp += obj_fun_w[pos] * variables[pos]\n",
    "            model2 += temp\n",
    "            model2.solve(PULP_CBC_CMD(msg=False))\n",
    "            if model_debug:\n",
    "                print(model2)\n",
    "                model_debug=False\n",
    "            #print(model2.objective.value())\n",
    "\n",
    "            if model2.objective.value()>=0:  #if the solution of the LP has non-negative solution t f-dominates s\n",
    "                is_dominated = True\n",
    "                break\n",
    "        if is_dominated:\n",
    "            continue\n",
    "        nd.append(s)\n",
    "    return nd\n",
    "\n",
    "#Third ND algorithm: BNL with VE + 2 phases: UVE2\n",
    "def uve2(data, vertices, p):\n",
    "    #PHASE 1\n",
    "    start = time.time()\n",
    "    skyline = find_skyline_bnl(data)\n",
    "    #print(nd)\n",
    "    end = time.time() - start\n",
    "    print('Number of points in the skyline ' + str(len(skyline)))\n",
    "    print('Time taken ' + str(end))\n",
    "    #PHASE 2\n",
    "    #print(skyline)\n",
    "    nd = skyline.copy()\n",
    "    \n",
    "    #count = 0\n",
    "    for s in skyline:\n",
    "        #count += 1\n",
    "        #print(count)\n",
    "        is_dominated = False\n",
    "        left_hand_side = computeInequality(s, vertices, p)\n",
    "        #print(s, left_hand_side)\n",
    "        for t in nd:\n",
    "            if s==t:\n",
    "                continue\n",
    "            right_hand_side = computeInequality(t, vertices, p)\n",
    "            #print(t, right_hand_side)\n",
    "            for k in range(len(right_hand_side)):  #if s satisfies all the inequalities, t f-dominates s otherwise no\n",
    "                is_dominated = True\n",
    "                if (left_hand_side[k] < right_hand_side[k]):\n",
    "                    is_dominated = False\n",
    "                    break\n",
    "            if is_dominated:\n",
    "                break\n",
    "        if is_dominated:\n",
    "            #print(s)\n",
    "            nd.remove(s)\n",
    "    return nd\n",
    "\n",
    "#4° ND algorithm: BNL with VE + 2 phases: SVE2\n",
    "def sve2(data, vertices, p):\n",
    "    #PHASE 1\n",
    "    start = time.time()\n",
    "    centr =  compute_centroid(vertices)\n",
    "    print(centr)\n",
    "    skyline = find_skyline_sfs(data, centr)\n",
    "    #print(nd)\n",
    "    end = time.time() - start\n",
    "    print('Number of points in the skyline ' + str(len(skyline)))\n",
    "    print('Time taken ' + str(end))\n",
    "    #PHASE 2\n",
    "    #print(skyline)\n",
    "    nd = []\n",
    "    #count = 0\n",
    "    for s in skyline: #candidate F-dominated tuple\n",
    "#         count += 1\n",
    "#         print(count)\n",
    "        is_dominated = False\n",
    "        left_hand_side = computeInequality(s, vertices, p)\n",
    "        #print(s, left_hand_side)\n",
    "        for t in nd: #candidate F-dominant tuple\n",
    "            right_hand_side = computeInequality(t, vertices, p)\n",
    "            #print(t, right_hand_side)\n",
    "            for k in range(len(right_hand_side)): #if s satisfies all the inequalities, t f-dominates s otherwise no\n",
    "                is_dominated = True\n",
    "                if (left_hand_side[k] < right_hand_side[k]):\n",
    "                    is_dominated = False\n",
    "                    break\n",
    "            if is_dominated:\n",
    "                #print(str(t) + ' dominates '+ str(s))\n",
    "                break\n",
    "        if is_dominated:\n",
    "            continue\n",
    "        nd.append(s)\n",
    "    return nd\n",
    "\n",
    "#5° ND algorithm: BNL with VE + 1 phase: SVE1\n",
    "def sve1(data, vertices, p):\n",
    "    #PHASE 1\n",
    "    if not isinstance(data,list):\n",
    "        data = list(data)\n",
    "\n",
    "    nd = []\n",
    "    #count = 0\n",
    "    centr =  compute_centroid(vertices)\n",
    "    #print(centr)\n",
    "    data_sorted = data.copy()\n",
    "    data_sorted.sort(key=lambda x: sort_function(x, centr))\n",
    "    for s in data_sorted: #candidate F-dominated tuple\n",
    "        is_dominated = False\n",
    "        for t in nd: #candidate F-dominant tuple\n",
    "            if dominates(t,s):  # if true, t dominates s\n",
    "                is_dominated = True\n",
    "                break\n",
    "        if is_dominated:\n",
    "            continue\n",
    "        left_hand_side = computeInequality(s, vertices, p)\n",
    "        for t in nd:  #candidate F-dominant tuple\n",
    "            right_hand_side = computeInequality(t, vertices, p)\n",
    "            #print(t, right_hand_side)\n",
    "            for k in range(len(right_hand_side)): #if s satisfies all the inequalities, t F-dominates s otherwise no\n",
    "                is_dominated = True\n",
    "                if (left_hand_side[k] < right_hand_side[k]):\n",
    "                    is_dominated = False\n",
    "                    break\n",
    "            if is_dominated:\n",
    "                #print(str(t) + ' dominates '+ str(s))\n",
    "                break\n",
    "        if is_dominated:\n",
    "            continue\n",
    "#         count += 1\n",
    "#         print(count)\n",
    "        nd.append(s)            \n",
    "    \n",
    "    return nd\n",
    "\n",
    "#6° ND algorithm: BNL with VE + 1 phase (Variant): SVE1F\n",
    "def sve1f(data, vertices, p):\n",
    "    #PHASE 1\n",
    "    if not isinstance(data,list):\n",
    "        data = list(data)\n",
    "\n",
    "    nd = []\n",
    "    #count = 0\n",
    "    centr =  compute_centroid(vertices)\n",
    "    #print(centr)\n",
    "    data_sorted = data.copy()\n",
    "    data_sorted.sort(key=lambda x: sort_function(x, centr))\n",
    "    \n",
    "    for s in data_sorted: #candidate F-dominated tuple\n",
    "        is_dominated = False\n",
    "        left_hand_side = computeInequality(s, vertices, p)\n",
    "        for t in nd: #candidate F-dominant tuple\n",
    "            \n",
    "            #print(t, right_hand_side)\n",
    "            if dominates(t,s): #if true, t dominates s\n",
    "                is_dominated  = True\n",
    "                break\n",
    "            right_hand_side = computeInequality(t, vertices, p)\n",
    "            for k in range(len(right_hand_side)): #if s satisfies all the inequalities, t F-dominates s otherwise no\n",
    "                is_dominated = True\n",
    "                if (left_hand_side[k] < right_hand_side[k]):\n",
    "                    is_dominated = False\n",
    "                    break\n",
    "            if is_dominated:\n",
    "                break\n",
    "        if is_dominated:\n",
    "            continue\n",
    "#         count += 1\n",
    "#         print(count)\n",
    "        nd.append(s)            \n",
    "    \n",
    "    return nd\n",
    "\n",
    "#PO Algorithm: version with PuLP optimizer and dual computation of po\n",
    "def POND(po, vertices, p):\n",
    "    if not isinstance(po,list):\n",
    "        po = list(po)\n",
    "\n",
    "    delta = 2\n",
    "    lastRound = False\n",
    "    minimum = 0\n",
    "    \n",
    "    while(not lastRound):\n",
    "        \n",
    "        if delta >= (len(po) - 1):\n",
    "            lastRound = True\n",
    "        po_reversed = po.copy()\n",
    "        po_reversed.reverse()\n",
    "        for t in po_reversed: #candidate F-dominated tuple\n",
    "            model = LpProblem(sense=LpMinimize)\n",
    "            variables= []\n",
    "            minimum = min(delta, len(po)-1)\n",
    "            if minimum == 0:\n",
    "                break\n",
    "                \n",
    "            #Prepare the LP model\n",
    "            variables = getVariables(minimum)\n",
    "            temp = 0\n",
    "            for i in range(len(variables)):\n",
    "                temp += variables[i]\n",
    "            model += temp == 1\n",
    "            \n",
    "            po_temp = []\n",
    "            po_temp = po.copy()\n",
    "            po_temp.remove(t)\n",
    "            \n",
    "            for l in range(len(vertices)): #compute the inequalities and add it to the LP model\n",
    "                left_side = 0\n",
    "                right_side = 0\n",
    "                for i in range(len(vertices[l])):    \n",
    "                    left_side += vertices[l][i] * computeLeftSide(variables,po_temp,i,p)\n",
    "                    right_side += vertices[l][i]* (t[i]**p)\n",
    "                model += left_side <= right_side\n",
    "#             print(minimum)\n",
    "#             print(po_temp)\n",
    "#             print(t)\n",
    "#             print(model)\n",
    "            model.solve(PULP_CBC_CMD(msg=False))\n",
    "            #print(LpStatus[model.status])\n",
    "            if (not (LpStatus[model.status] == 'Infeasible')): # PO contains t iff the linear system is infeasible, otherwise is F-dominated by a convex combination of the other\n",
    "#                 print(\"Removed \"+ str(t))\n",
    "                po.remove(t)\n",
    "                #po_reversed.remove(t)\n",
    "            del model\n",
    "        \n",
    "        delta = delta * 2\n",
    "    \n",
    "    return po\n",
    "\n",
    "def po_primal_pulp(po, constraints, k_):\n",
    "    if not isinstance(po,list):\n",
    "        po = list(po)\n",
    "\n",
    "    if len(po) == 0:\n",
    "        return []\n",
    "\n",
    "    delta = 2\n",
    "    lastRound = False\n",
    "    dimensions = len(po[0])\n",
    "\n",
    "    while(not lastRound):\n",
    "\n",
    "        if delta >= (len(po) - 1):\n",
    "            lastRound = True\n",
    "        po_reversed = po.copy()\n",
    "        po_reversed.reverse()\n",
    "        for t in po_reversed: #candidate F-dominated tuple\n",
    "            model = LpProblem(sense=LpMaximize)\n",
    "            #Prepare the LP model\n",
    "            variables = getVariables_primal(dimensions)\n",
    "\n",
    "            temp2 = 0\n",
    "            for cons in constraints:\n",
    "                i = 0\n",
    "                for index in range(len(cons)):\n",
    "                    temp2 += cons[index]*variables[index+1]\n",
    "                model += (temp2 <= k_[i])\n",
    "                i += 1\n",
    "\n",
    "            minimum = min(delta, len(po)-1)\n",
    "            if minimum == 0:\n",
    "                break\n",
    "\n",
    "            temp = 0\n",
    "            for i in range(len(variables)-1):\n",
    "                temp += variables[i+1]\n",
    "\n",
    "            model += temp == 1\n",
    "\n",
    "            po_temp = po.copy()\n",
    "            po_temp.remove(t)\n",
    "\n",
    "            for j in range(minimum): #compute the inequalities and add it to the LP model\n",
    "                left_side = 0\n",
    "                for i in range(dimensions):\n",
    "                    left_side += variables[i+1] * (t[i]-po_temp[j][i])\n",
    "                model += (left_side + variables[0] <= 0)\n",
    "            model.setObjective(variables[0])\n",
    "            model.solve(PULP_CBC_CMD(msg=False))\n",
    "\n",
    "            if (model.objective.value() > 0): # PO contains t iff the linear system is infeasible, otherwise is F-dominated by a convex combination of the other\n",
    "#                 print(\"Removed \"+ str(t))\n",
    "                continue\n",
    "                #po_reversed.remove(t)\n",
    "            po.remove(t)\n",
    "            del model\n",
    "\n",
    "        delta = delta * 2\n",
    "\n",
    "    return po\n",
    "\n",
    "def po_primal_pulp_non_inc(po, constraints, k_):\n",
    "    if not isinstance(po,list):\n",
    "        po = list(po)\n",
    "\n",
    "    if len(po) == 0:\n",
    "        return []\n",
    "    dimensions = len(po[0])\n",
    "    po_reversed = po.copy()\n",
    "    po_reversed.reverse()\n",
    "    for t in po_reversed:\n",
    "        model = LpProblem(sense=LpMaximize)\n",
    "        #Prepare the LP model\n",
    "        variables = getVariables_primal(dimensions)\n",
    "        temp2 = 0\n",
    "        for cons in constraints:\n",
    "            i = 0\n",
    "            for index in range(len(cons)):\n",
    "                temp2 += cons[index]*variables[index+1]\n",
    "            model += (temp2 <= k_[i])\n",
    "            i += 1\n",
    "        temp = 0\n",
    "        for i in range(len(variables)-1):\n",
    "            temp += variables[i+1]\n",
    "\n",
    "        model += temp == 1\n",
    "        po_temp = po.copy()\n",
    "        po_temp.remove(t)\n",
    "        for j in range(len(po_temp)): #compute the inequalities and add it to the LP model\n",
    "            left_side = 0\n",
    "            for i in range(dimensions):\n",
    "                left_side += variables[i+1] * (t[i]-po_temp[j][i])\n",
    "            model += (left_side + variables[0] <= 0)\n",
    "        model.setObjective(variables[0])\n",
    "        model.solve(PULP_CBC_CMD(msg=False))\n",
    "\n",
    "        if (model.objective.value() > 0): # PO contains t iff the linear system is infeasible, otherwise is F-dominated by a convex combination of the other\n",
    "#                 print(\"Removed \"+ str(t))\n",
    "            continue\n",
    "            #po_reversed.remove(t)\n",
    "        po.remove(t)\n",
    "        del model\n",
    "\n",
    "    return po\n",
    "\n",
    "#PO Algorithm: version with gurobipy optimizer and dual computation of po \n",
    "def po_dual(po, vertices, p):\n",
    "    if not isinstance(po,list):\n",
    "        po = list(po)\n",
    "        \n",
    "    delta = 2\n",
    "    lastRound = False\n",
    "    minimum = 0\n",
    "\n",
    "    count_round = 0\n",
    "    count_lp = 0\n",
    "    while(not lastRound):\n",
    "        count_round +=1\n",
    "        if delta >= (len(po) - 1):\n",
    "            lastRound = True\n",
    "        \n",
    "        po_reversed = po.copy()\n",
    "        po_reversed.reverse()\n",
    "        \n",
    "        for t in po_reversed: #candidate F-dominated tuple\n",
    "            variables= []\n",
    "            minimum = min(delta, len(po)-1)\n",
    "            if minimum == 0:\n",
    "                break\n",
    "            count_lp +=1\n",
    "            #Prepare the LP model\n",
    "            m, variables = getModel_dual(minimum)\n",
    "            temp = 0\n",
    "            for i in range(len(variables)):\n",
    "                temp += variables[i]            \n",
    "            m.addConstr(temp == 1)\n",
    "            \n",
    "            # first min(σ , |PO| − 1) tuples in PO \\ {t}\n",
    "            po_temp = []\n",
    "            po2 = [s for s in po if s != t]\n",
    "            for k in range(minimum):\n",
    "                    po_temp.append(po2[k])\n",
    "            for l in range(len(vertices)): #compute the inequalities and add it to the LP model\n",
    "                left_side = 0\n",
    "                right_side = 0\n",
    "                for i in range(len(vertices[l])):    \n",
    "                    left_side += vertices[l][i] * computeLeftSide(variables,po_temp,i,p)\n",
    "                    right_side += vertices[l][i]* (t[i]**p)\n",
    "                m.addConstr(left_side <= right_side)\n",
    "        \n",
    "            m.optimize()\n",
    "            \n",
    "            if (not (m.status == 3)): # PO contains t iff the linear system is infeasible, otherwise is F-dominated by a convex combination of the other\n",
    "                po.remove(t)\n",
    "        \n",
    "        delta = delta * 2\n",
    "    \n",
    "#     print('Total round: '+ str(count_round))\n",
    "#     print('Total lp: '+ str(count_lp))\n",
    "#     print('Last value: '+ str(minimum))\n",
    "    return po\n",
    "\n",
    "#PO Algorithm: version with gurobipy optimizer and primal computation of po \n",
    "def po_primal(po, constraints, k_):\n",
    "\n",
    "    if not isinstance(po,list):\n",
    "        po = list(po)\n",
    "        \n",
    "    if len(po) == 0:\n",
    "        return []\n",
    "    delta = 2\n",
    "    lastRound = False\n",
    "    minimum = 0\n",
    "\n",
    "    count_round = 0\n",
    "    count_lp = 0\n",
    "    \n",
    "    \n",
    "    dimensions = len(po[0])\n",
    "    #Prepare the LP model \n",
    "    m, variables, var_names = getModel_primal(dimensions)\n",
    "\n",
    "    m.setObjective(variables[0], gp.GRB.MAXIMIZE)\n",
    "    temp = 0\n",
    "    for i in range(len(variables)-1):\n",
    "        temp += variables[i+1]            \n",
    "    m.addConstr(temp == 1)\n",
    "\n",
    "    temp2 = 0\n",
    "    for cons in constraints:\n",
    "        i = 0\n",
    "        for index in range(len(cons)):\n",
    "            temp2 += cons[index]*variables[index+1]\n",
    "        m.addConstr(temp2 <= k_[i])\n",
    "        i += 1\n",
    "    \n",
    "    while(not lastRound):\n",
    "        count_round +=1\n",
    "        if delta >= (len(po) - 1):\n",
    "            lastRound = True\n",
    "        \n",
    "        po_reversed = po.copy()\n",
    "        po_reversed.reverse()\n",
    "        \n",
    "        for t in po_reversed: #candidate F-dominated tuple\n",
    "            \n",
    "            minimum = min(delta, len(po)-1)\n",
    "            if minimum == 0:\n",
    "                break\n",
    "            count_lp +=1  \n",
    "            \n",
    "            m.update()\n",
    "            m_temp = m.copy()\n",
    "            \n",
    "            \n",
    "            # first min(σ , |PO| − 1) tuples in PO \\ {t}\n",
    "            po_temp = []\n",
    "            po2 = [s for s in po if s != t]\n",
    "            for k in range(minimum):\n",
    "                    po_temp.append(po2[k])\n",
    "            for j in range(minimum): #compute the inequalities and add it to the LP model\n",
    "                left_side = 0\n",
    "                for i in range(dimensions): \n",
    "                    left_side += m_temp.getVarByName(var_names[i+1])*(t[i]-po_temp[j][i])\n",
    "                m_temp.addConstr(left_side + m_temp.getVarByName(var_names[0]) <= 0)\n",
    "#             m_temp.update()\n",
    "            m_temp.optimize()\n",
    "            if (m_temp.status == 2) and (m_temp.objVal > 0): # PO contains t iff the linear system is infeasible, otherwise is F-dominated by a convex combination of the other\n",
    "                continue\n",
    "            \n",
    "            po.remove(t)\n",
    "                    \n",
    "            del m_temp\n",
    "        delta = delta * 2\n",
    "    \n",
    "    return po"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac2cde03",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function that computes the index of the tuples\n",
    "\n",
    "#Angular partitioning\n",
    "def get_angular_partitionIndex(datapoint, dimensions, numSlices = 2):\n",
    "    \n",
    "    angle = 0\n",
    "    for i in range(dimensions):\n",
    "        angle = angle + datapoint[i]**2\n",
    "        \n",
    "    anglesArray = []\n",
    "    ## first is radius then all angles\n",
    "    for i in range(dimensions):\n",
    "        if i == 0:\n",
    "            # radius\n",
    "            continue\n",
    "        else:\n",
    "            angle = angle - (datapoint[i-1]**2) \n",
    "            angle = max(0,angle)\n",
    "            if datapoint[i-1] == 0:\n",
    "                value = sqrt(angle) / (0.0001)\n",
    "            else:\n",
    "                value = sqrt(angle) /  datapoint[i-1]\n",
    "        anglesArray.append(value)\n",
    "        \n",
    "    index = 0\n",
    "    for i in range(len(anglesArray)):\n",
    "        index = index + floor(atan(anglesArray[i])*(2/pi)*numSlices) * (numSlices**i)\n",
    "    return index\n",
    "\n",
    "#Grid partitioning\n",
    "def get_grid_partition_index(datapoint, numSlices = 2):\n",
    "    \n",
    "    index = 0\n",
    "    for i in range(len(datapoint)):\n",
    "        # Maps space from 0 to numSlices ^ dimensions - 1\n",
    "        if datapoint[i] >= 1:\n",
    "            index = index + (numSlices-1) * (numSlices**i)\n",
    "        else:\n",
    "            index = index + floor(datapoint[i] * numSlices) * (numSlices**i)\n",
    "    return index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef4978a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid partitioning\n",
    "print('Grid partitioning with a serial grid filtering phase')\n",
    "class Container:\n",
    "    def __init__(self, worstPoint = [], bestPoint = [], dataContained = []):\n",
    "        #worst tuple\n",
    "        self.worstPoint = worstPoint\n",
    "        #best tuple\n",
    "        self.bestPoint = bestPoint\n",
    "        self.dataContained = dataContained\n",
    "        \n",
    "    def addPoint(self, dataPoint):\n",
    "        if len(dataPoint) != len(self.worstPoint):\n",
    "            raise Exception('Datapoint dimension not consistent with container point`s dimensions: ' \\\n",
    "                            + str(len(dataPoint)) + \\\n",
    "                            ' ' + str(len(self.worstPoint)))\n",
    "        self.dataContained.add(dataPoint)\n",
    "\n",
    "            \n",
    "def filtering_containers(containerList):\n",
    "    nd = []\n",
    "    containerList.sort(key=lambda x: (min(x.bestPoint)))\n",
    "    for container in containerList:\n",
    "        if not container.dataContained: #if dataContained array is not empty\n",
    "            continue\n",
    "        bp = container.bestPoint\n",
    "        dominated = False\n",
    "        for other in nd:\n",
    "            if container == other:\n",
    "                continue\n",
    "            #if the best point of the selected container is dominated by the worstPoint of one of the other containers \n",
    "            if dominates(other.worstPoint, bp):\n",
    "                dominated = True\n",
    "                break \n",
    "        if dominated:\n",
    "            continue \n",
    "        nd.append(container)\n",
    "    \n",
    "    return nd\n",
    "    \n",
    "# Finds the skyline of grid containers based on its representative point min and max\n",
    "def query_containers(datapoints, numSlicesPerDimension = 8):\n",
    "    \n",
    "    limit = 1 / (numSlicesPerDimension)\n",
    "    #print('iterating - limit: ' +str(limit))\n",
    "    dimensions = len(datapoints[0])\n",
    "    num_slices_in_space = numSlicesPerDimension**dimensions\n",
    "    containerList = []\n",
    "    # create N square containers with each container having the datapoints contained and an index\n",
    "    for i in range(num_slices_in_space): \n",
    "        worst = []\n",
    "        best =[]\n",
    "        for j in range(dimensions):\n",
    "            #inizializzazione worst tuple\n",
    "            index_w = floor( i / (numSlicesPerDimension**j) ) % numSlicesPerDimension\n",
    "            worst.insert(j, index_w * limit + limit)\n",
    "            #inizializzazione best tuple\n",
    "            index_b = floor( i / (numSlicesPerDimension**j) ) % numSlicesPerDimension\n",
    "            best.insert(j, index_b * limit)\n",
    "                \n",
    "        containerList.insert(i, Container(worst, best, []))\n",
    "        \n",
    "    for dp in datapoints:\n",
    "        index = 0\n",
    "        for i in range(len(dp)):\n",
    "            if dp[i] >= 1:\n",
    "                index = index + floor(0.99999 / limit) * (numSlicesPerDimension**i)\n",
    "            else:\n",
    "                index = index + floor(dp[i] / limit) * (numSlicesPerDimension**i)\n",
    "        (containerList[index].dataContained).append(dp)\n",
    "\n",
    "    print(\"Number of containers before filtering: \" + str(len(containerList)))\n",
    "    resultingContainers = filtering_containers(containerList)\n",
    "    input_list = []\n",
    "    for container in resultingContainers:\n",
    "        input_list = input_list + container.dataContained\n",
    "    print('Number of points after filtering: '+ str(len(input_list)))\n",
    "    return input_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa27c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Helper methods for Representative filtering.')\n",
    "def get_best_representatives(index, dataset, n = 30):\n",
    "    # index is the dimension which we should check\n",
    "    # is a tuple with area_covered as the first element and the n-dimensional point as the second element\n",
    "\n",
    "    best_n_points = []\n",
    "    limit = 1/n # default is 0.01\n",
    "    for i in range(n):\n",
    "        best_n_points.append((0,[]))\n",
    "    counter = 0\n",
    "    for point in dataset:\n",
    "        counter = counter + 1\n",
    "        area_covered = 1\n",
    "        for value in point:\n",
    "            area_covered = area_covered * (1-value)\n",
    "        rep_index = floor(point[index]/limit)\n",
    "        if best_n_points[rep_index][0] < area_covered:\n",
    "            best_n_points[rep_index] = (area_covered, point)\n",
    "    best_n_points = [x[1] for x in best_n_points if x[1]]\n",
    "    return best_n_points\n",
    "\n",
    "\n",
    "def filter_with_memory(datapoints, reps, weights, onlyFilter = True):\n",
    "    \n",
    "    nd = []\n",
    "#     comp = []\n",
    "#     comp.insert(0,0) \n",
    "    for ps in datapoints:\n",
    "        # ps : potentially non dominated point\n",
    "        dominated = False\n",
    "        for rep in reps:\n",
    "            #comp[0] = comp[0] +1\n",
    "            if dominates(rep, ps):\n",
    "                dominated = True # other point is dominated\n",
    "                break\n",
    "        if not dominated:\n",
    "            nd.append(ps)\n",
    "    \n",
    "    if onlyFilter:\n",
    "        return nd\n",
    "    return find_skyline_sfs(nd, weights)\n",
    "\n",
    "def filter_with_improvement(datapoints, reps, weights, onlyFilter = False):\n",
    "    \n",
    "    nd = []\n",
    "        \n",
    "    comp = []\n",
    "    comp.insert(0,0) \n",
    "    \n",
    "    for ps in datapoints:\n",
    "        # ps : potentially non dominated point\n",
    "        dominated = False\n",
    "        for rep in reps:\n",
    "            comp[0] = comp[0] +1\n",
    "            if ps == rep: \n",
    "                break\n",
    "            if dominates(rep, ps):\n",
    "                dominated = True # other point is dominated\n",
    "                break\n",
    "            \n",
    "        if not dominated:\n",
    "            nd.append(ps)\n",
    "    \n",
    "    if onlyFilter:\n",
    "        return comp\n",
    "    return find_skyline_sfs(nd, weights)\n",
    "def filter_nd_with_memory2(datapoints, reps, vertices, p, onlyFilter = False):\n",
    "    \n",
    "    if not isinstance(datapoints,list):\n",
    "        datapoints = list(datapoints)\n",
    "    \n",
    "    nd = []\n",
    "    #centr =  compute_centroid(vertices)\n",
    "    #print(centr)\n",
    "    #datapoints.sort(key=lambda x: sort_function(x, centr))\n",
    "    \n",
    "    for s in datapoints:\n",
    "        # ps : potentially non dominated point\n",
    "        is_dominated = False\n",
    "        for rep in reps:\n",
    "            if dominates(rep, s):\n",
    "                is_dominated = True # other point is dominated\n",
    "                break\n",
    "        if is_dominated:\n",
    "            continue\n",
    "        \n",
    "        left_hand_side = computeInequality(s, vertices, p)\n",
    "        for t in reps: #candidate F-dominant tuple\n",
    "            #print(t, right_hand_side)\n",
    "            if t==s:\n",
    "                break\n",
    "            right_hand_side = computeInequality(t, vertices, p)\n",
    "            for k in range(len(right_hand_side)): #if s satisfies all the inequalities, t F-dominates s otherwise no\n",
    "                is_dominated = True\n",
    "                if (left_hand_side[k] < right_hand_side[k]):\n",
    "                    is_dominated = False\n",
    "                    break\n",
    "            if is_dominated:\n",
    "                break\n",
    "        if is_dominated:\n",
    "            continue\n",
    "        nd.append(s)   \n",
    "    \n",
    "    if onlyFilter:\n",
    "        return nd\n",
    "    return sve1f(nd, vertices, p)\n",
    "\n",
    "\n",
    "def filter_nd_with_memory(datapoints, reps, vertices, p, onlyFilter = False):\n",
    "    \n",
    "    nd = []\n",
    "#     centr =  compute_centroid(vertices)\n",
    "#     #print(centr)\n",
    "#     datapoints = datapoints.copy()\n",
    "#     datapoints.sort(key=lambda x: sort_function(x, centr))\n",
    "    \n",
    "#     for s in datapoints:\n",
    "#         # ps : potentially non dominated point\n",
    "#         is_dominated = False\n",
    "#         left_hand_side = computeInequality(s, vertices, p)\n",
    "#         for t in reps: #candidate F-dominant tuple\n",
    "#             #print(t, right_hand_side)\n",
    "#             if dominates(t,s): #if true, t dominates s\n",
    "#                 is_dominated  = True\n",
    "#                 break\n",
    "#             right_hand_side = computeInequality(t, vertices, p)\n",
    "#             for k in range(len(right_hand_side)): #if s satisfies all the inequalities, t F-dominates s otherwise no\n",
    "#                 is_dominated = True\n",
    "#                 if (left_hand_side[k] < right_hand_side[k]):\n",
    "#                     is_dominated = False\n",
    "#                     break\n",
    "#             if is_dominated:\n",
    "#                 break\n",
    "#         if is_dominated:\n",
    "#             continue\n",
    "#         nd.append(s)   \n",
    "    for ps in datapoints:\n",
    "        # ps : potentially non dominated point\n",
    "        dominated = False\n",
    "        for rep in reps:\n",
    "            if dominates(rep, ps):\n",
    "                dominated = True # other point is dominated\n",
    "                break\n",
    "        if not dominated:\n",
    "            nd.append(ps)\n",
    "    \n",
    "    return sve1f(nd, vertices, p)\n",
    "\n",
    "\n",
    "#Function that finds the representatives in parallel \n",
    "def parallel_representative_filtering(dataAsList, weights, slicesForSorting = 12, onlyFilter = True, numberReps = 30):\n",
    "    \n",
    "    start_indexing = time.time()\n",
    "\n",
    "    representatives = spark.sparkContext.parallelize(dataAsList, len(dataAsList[0]) ) \\\n",
    "                                    .mapPartitionsWithIndex(lambda index, y: get_best_representatives(index, y, n = numberReps)) \\\n",
    "                                    .collect()\n",
    "    end_indexing = time.time() - start_indexing\n",
    "    print('Time taken to find best reps ' + str(end_indexing))\n",
    "    print('Length of representatives: ' + str(len(representatives)))\n",
    "    representatives = find_skyline_sfs(representatives, weights)\n",
    "    print('Length of representatives after skyline query: ' + str(len(representatives)))\n",
    "    \n",
    "    start_parallel = time.time()\n",
    "    parallel_skyline = []\n",
    "    \n",
    "    parallel_skyline = spark.sparkContext.parallelize(dataAsList, slicesForSorting)\\\n",
    "                                .mapPartitions(lambda x : filter_with_memory(x, representatives, weights, onlyFilter)) \\\n",
    "                                .collect()\n",
    "    end_parallel = time.time() - start_parallel\n",
    "    #print('Number of comparisons: '+ str(parallel_skyline))\n",
    "    print('Time taken to filter: ' +str(end_parallel))\n",
    "    print('Length of the data after filter: ' + str(len(parallel_skyline)))\n",
    "    \n",
    "    return parallel_skyline\n",
    "\n",
    "def parallel_representative_filtering_angular(dataAsList, weights, numSlices = 5, onlyFilter = True, numberReps = 30):\n",
    "\n",
    "    start_indexing = time.time()\n",
    "\n",
    "    representatives = spark.sparkContext.parallelize(dataAsList, len(dataAsList[0]) ) \\\n",
    "                                    .mapPartitionsWithIndex(lambda index, y: get_best_representatives(index, y, n = numberReps)) \\\n",
    "                                    .collect()\n",
    "    end_indexing = time.time() - start_indexing\n",
    "    print('Time taken to find best reps ' + str(end_indexing))\n",
    "    print('Length of representatives: ' + str(len(representatives)))\n",
    "    representatives = find_skyline_sfs(representatives, weights)\n",
    "    print('Length of representatives after skyline query: ' + str(len(representatives)))\n",
    "\n",
    "    start_parallel = time.time()\n",
    "    parallel_skyline = []\n",
    "    dimensions = len(dataAsList[0])\n",
    "    parallel_skyline = spark.sparkContext.parallelize(dataAsList) \\\n",
    "                    .map(lambda x : ( get_angular_partitionIndex(x, dimensions, numSlices)  , x) )  \\\n",
    "                    .partitionBy(numSlices**(dimensions-1)) \\\n",
    "                    .mapPartitions(lambda x : execute_filter_with_memory(x, representatives, weights, onlyFilter), preservesPartitioning=True)  \\\n",
    "                    .collect()\n",
    "    end_parallel = time.time() - start_parallel\n",
    "    #print('Number of comparisons: '+ str(parallel_skyline))\n",
    "    print('Time taken to filter: ' +str(end_parallel))\n",
    "    print('Length of the data after filter: ' + str(len(parallel_skyline)))\n",
    "\n",
    "    return parallel_skyline\n",
    "\n",
    "def parallel_representative_filtering_imp(dataAsList, weights, slicesForSorting = 12, onlyFilter = True, givenReps = [], numberReps = 30):\n",
    "    \n",
    "    start_indexing = time.time()\n",
    "    if not givenReps:\n",
    "        representatives = spark.sparkContext.parallelize(dataAsList, len(dataAsList[0]) ) \\\n",
    "                                    .mapPartitionsWithIndex(lambda index, y: get_best_representatives(index, y, n = numberReps)) \\\n",
    "                                    .collect()\n",
    "    else:\n",
    "        representatives = givenReps\n",
    "    end_indexing = time.time() - start_indexing\n",
    "    print('Time taken to find best reps ' + str(end_indexing))\n",
    "    print('Length of representatives: ' + str(len(representatives)))\n",
    "    representatives = find_skyline_sfs(representatives, weights)\n",
    "    print('Length of representatives after skyline query: ' + str(len(representatives)))\n",
    "    \n",
    "    start_parallel = time.time()\n",
    "    parallel_skyline = []\n",
    "    \n",
    "    parallel_skyline = spark.sparkContext.parallelize(dataAsList, slicesForSorting)\\\n",
    "                                .mapPartitions(lambda x : filter_with_improvement(x, representatives, weights, onlyFilter)) \\\n",
    "                                .collect()\n",
    "    \n",
    "    end_parallel = time.time() - start_parallel\n",
    "    print('Number of comparisons: '+ str(parallel_skyline))\n",
    "    print('Time taken to filter: ' +str(end_parallel))\n",
    "    print('Length of the data after filter: ' + str(len(parallel_skyline)))\n",
    "    \n",
    "    return parallel_skyline\n",
    "\n",
    "def parallel_representative_filtering_nd(dataAsList, vertices,p, slicesForSorting = 12, onlyFilter = True, givenReps = [], numberReps = 30):\n",
    "\n",
    "    start_indexing = time.time()\n",
    "    if not givenReps:\n",
    "        representatives = spark.sparkContext.parallelize(dataAsList, len(dataAsList[0]) ) \\\n",
    "                                    .mapPartitionsWithIndex(lambda index, y: get_best_representatives(index, y, n = numberReps)) \\\n",
    "                                    .collect()\n",
    "    else:\n",
    "        representatives = givenReps\n",
    "    end_indexing = time.time() - start_indexing\n",
    "    print('Time taken to find best reps ' + str(end_indexing))\n",
    "    print('Length of representatives: ' + str(len(representatives)))\n",
    "    representatives = sve1f(representatives, vertices, p)\n",
    "    print('Length of representatives after ND query: ' + str(len(representatives)))\n",
    "\n",
    "    start_parallel = time.time()\n",
    "    parallel_skyline = []\n",
    "\n",
    "    parallel_skyline = spark.sparkContext.parallelize(dataAsList, slicesForSorting)\\\n",
    "                                .mapPartitions(lambda x : filter_nd_with_memory2(x, representatives, vertices, p, onlyFilter)) \\\n",
    "                                .collect()\n",
    "    end_parallel = time.time() - start_parallel\n",
    "    #print('Number of comparisons: '+ str(parallel_skyline))\n",
    "    print('Time taken to filter: ' +str(end_parallel))\n",
    "    print('Length of the data after filter: ' + str(len(parallel_skyline)))\n",
    "\n",
    "    return parallel_skyline\n",
    "def parallel_representative_filtering_nd_angular(dataAsList, vertices, p, numSlices = 5, onlyFilter = True, numberReps = 30):\n",
    "\n",
    "    start_indexing = time.time()\n",
    "\n",
    "    representatives = spark.sparkContext.parallelize(dataAsList, len(dataAsList[0]) ) \\\n",
    "                                    .mapPartitionsWithIndex(lambda index, y: get_best_representatives(index, y, n = numberReps)) \\\n",
    "                                    .collect()\n",
    "    end_indexing = time.time() - start_indexing\n",
    "    print('Time taken to find best reps ' + str(end_indexing))\n",
    "    print('Length of representatives: ' + str(len(representatives)))\n",
    "    representatives = sve1f(representatives, vertices, p)\n",
    "    print('Length of representatives after skyline query: ' + str(len(representatives)))\n",
    "\n",
    "    start_parallel = time.time()\n",
    "    parallel_skyline = []\n",
    "    dimensions = len(dataAsList[0])\n",
    "    parallel_skyline = spark.sparkContext.parallelize(dataAsList) \\\n",
    "                    .map(lambda x : ( get_angular_partitionIndex(x, dimensions, numSlices)  , x) )  \\\n",
    "                    .partitionBy(numSlices**(dimensions-1)) \\\n",
    "                    .mapPartitions(lambda x : execute_sve1_filter_with_memory(x, representatives, vertices, p, onlyFilter), preservesPartitioning=True)  \\\n",
    "                    .collect()\n",
    "    end_parallel = time.time() - start_parallel\n",
    "    #print('Number of comparisons: '+ str(parallel_skyline))\n",
    "    print('Time taken to filter: ' +str(end_parallel))\n",
    "    print('Length of the data after filter: ' + str(len(parallel_skyline)))\n",
    "\n",
    "    return parallel_skyline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb65b596",
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeList(l):\n",
    "\n",
    "    p =l.strip().split(\" \")\n",
    "    point=[]\n",
    "    for x in p:\n",
    "        point.append(float(x))\n",
    "    return point\n",
    "\n",
    "def parallel_3P(dataAsList, weights, numReps=30, slicesForSorting = 12):\n",
    "    #numReps = ceil(len(dataAsList)/1000)\n",
    "    sortedData = []\n",
    "    print('Slices for representative skyline ' + str(slicesForSorting))\n",
    "    start = time.time()\n",
    "    #merge sort on the first \n",
    "    #if len(dataAsList) <= 10**5:\n",
    "    #    dataAsList.sort(key = lambda x : x[0])\n",
    "    #    sortedData = dataAsList\n",
    "    #else:\n",
    "    #    sortedData = spark.sparkContext.parallelize(dataAsList, max(12,slicesForSorting)).sortBy(lambda x: x[0]).collect()\n",
    "    dataAsList.sort(key = lambda x : x[0])\n",
    "    sortedData = dataAsList\n",
    "    end = time.time() - start\n",
    "    print('Time taken for sorting: '+ str(end))\n",
    "    \n",
    "    start_indexing = time.time()\n",
    "    representatives = spark.sparkContext.parallelize(sortedData, len(dataAsList[0])) \\\n",
    "                                .mapPartitionsWithIndex(lambda index, y: get_best_representatives(index, y, numReps)) \\\n",
    "                                .collect()\n",
    "    end_indexing = time.time() - start_indexing\n",
    "    print('Length of representatives: ' + str(len(representatives)) + ', time taken to find: '+ str(end_indexing))\n",
    "\n",
    "    start_parallel = time.time()\n",
    "    parallel_skyline = spark.sparkContext.parallelize(sortedData, slicesForSorting)\\\n",
    "                                .mapPartitions(lambda x : filter_with_memory(x, representatives, weights, onlyFilter = False)) \\\n",
    "                                .collect()\n",
    "    end_parallel = time.time() - start_parallel\n",
    "    print('Length of the parallel section skyline: ' + str(len(parallel_skyline))+ ', time taken to find it: '+ str(end_parallel))\n",
    "    \n",
    "    return parallel_skyline\n",
    "\n",
    "def parallel_3P2(dataset, weights, slicesForSorting = 12):\n",
    "    #numReps = ceil(len(dataAsList)/1000)\n",
    "    sortedData = []\n",
    "    print('Slices for representative skyline ' + str(slicesForSorting))\n",
    "    start = time.time()\n",
    "    #merge sort on the first\n",
    "    rdd = spark.sparkContext.textFile(os.getcwd() + '/datasets' + dataset, minPartitions=500)\n",
    "    dataAsList = rdd.map(computeList).collect()\n",
    "    dataAsList = normalize_data(dataAsList).tolist()\n",
    "    print(len(dataAsList))\n",
    "    print(dataAsList[0:5])\n",
    "\n",
    "    start = time.time()\n",
    "    dataAsList.sort(key = lambda x : x[0])\n",
    "    #sortedData = spark.sparkContext.parallelize(dataAsList, slicesForSorting).mapPartitions().collect()\n",
    "\n",
    "    #sortedData = spark.sparkContext.parallelize(dataAsList, max(12,slicesForSorting)).sortBy(lambda x: x[0]).collect()\n",
    "    end = time.time() - start\n",
    "    print('Time taken for sorting: '+ str(end))\n",
    "\n",
    "    start_indexing = time.time()\n",
    "    representatives = spark.sparkContext.parallelize(dataAsList, len(dataAsList[0])) \\\n",
    "                                .mapPartitionsWithIndex(lambda index, y: get_best_representatives(index, y, 100)) \\\n",
    "                                .collect()\n",
    "    end_indexing = time.time() - start_indexing\n",
    "    print('Length of representatives: ' + str(len(representatives)) + ', time taken to find: '+ str(end_indexing))\n",
    "\n",
    "    start_parallel = time.time()\n",
    "    parallel_skyline = []\n",
    "    parallel_skyline = spark.sparkContext.parallelize(dataAsList, slicesForSorting)\\\n",
    "                                .mapPartitions(lambda x : filter_with_memory(x, representatives, weights)) \\\n",
    "                                .collect()\n",
    "    end_parallel = time.time() - start_parallel\n",
    "    print('Length of the parallel section skyline: ' + str(len(parallel_skyline))+ ', time taken to find it: '+ str(end_parallel))\n",
    "\n",
    "    return parallel_skyline\n",
    "\n",
    "def parallel_3P_sve1f(dataAsList, vertices, p, slicesForSorting = 12):\n",
    "    #numReps = ceil(len(dataAsList)/1000)\n",
    "    sortedData = []\n",
    "    print('Slices for representative skyline ' + str(slicesForSorting))\n",
    "    start = time.time()\n",
    "    #merge sort on the first \n",
    "    dataAsList.sort(key = lambda x : x[0])\n",
    "    sortedData = dataAsList\n",
    "    end = time.time() - start\n",
    "    print('Time taken for sorting: '+ str(end))\n",
    "    \n",
    "    start_indexing = time.time()\n",
    "    representatives = spark.sparkContext.parallelize(sortedData, len(dataAsList[0])) \\\n",
    "                                .mapPartitionsWithIndex(lambda index, y: get_best_representatives(index, y)) \\\n",
    "                                .collect()\n",
    "    end_indexing = time.time() - start_indexing\n",
    "    representatives = sve1f(representatives, vertices, p)\n",
    "    print('Length of representatives: ' + str(len(representatives)) + ', time taken to find: '+ str(end_indexing))\n",
    "\n",
    "    start_parallel = time.time()\n",
    "    parallel_skyline = []\n",
    "\n",
    "    parallel_skyline = spark.sparkContext.parallelize(sortedData, slicesForSorting)\\\n",
    "                                .mapPartitions(lambda x : filter_nd_with_memory2(x, representatives, vertices, p)) \\\n",
    "                                .collect()\n",
    "    end_parallel = time.time() - start_parallel\n",
    "    print('Length of the parallel section skyline: ' + str(len(parallel_skyline))+ ', time taken to find it: '+ str(end_parallel))\n",
    "    \n",
    "    return parallel_skyline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c1f6e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('BNL distributed version')\n",
    "\n",
    "#Random partitioning\n",
    "def parallel_bnl(data, numberOfSlices = 30):\n",
    "    start1 = time.time()\n",
    "\n",
    "    initialValues = spark.sparkContext.parallelize(data, numberOfSlices).mapPartitions(find_skyline_bnl).collect()\n",
    "    end = time.time() - start1\n",
    "    print(\"Length of the local skylines after parallel section is : \" + str(len(initialValues)) + \", time taken: \" + str(end))\n",
    "    skyline = find_skyline_bnl(initialValues)\n",
    "    end2 = time.time() - start1\n",
    "    print(\"Length of the skyline is : \" + str(len(skyline)) + \", total time taken: \" + str(end2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352ac7aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('SFS distributed computation')\n",
    "import os\n",
    "#Random partitioning\n",
    "def parallel_sfs(data, weights, numberOfSlices = 100):\n",
    "    start1 = time.time()\n",
    "\n",
    "    initialValues = spark.sparkContext.parallelize(data, numberOfSlices).mapPartitions(lambda x: find_skyline_sfs(x, weights)).collect()\n",
    "    end = time.time() - start1\n",
    "    print(\"Length of the local skylines after parallel section is : \" + str(len(initialValues)) + \", time taken: \" + str(end))\n",
    "\n",
    "    skyline = find_skyline_sfs(initialValues, weights)\n",
    "    end2 = time.time() - start1\n",
    "    print(\"Length of the skyline is : \" + str(len(skyline)) + \", total time taken: \" + str(end2))\n",
    "\n",
    "#One-slice partitioning\n",
    "\n",
    "def sliced_partitioning_sfs(datapoints, weights, numPartitions=100):\n",
    "    start = time.time()\n",
    "    datapoints.sort(key = lambda x : x[0])\n",
    "    sortedData = datapoints\n",
    "    end = time.time() - start\n",
    "    print('Time taken for sorting: '+ str(end))\n",
    "\n",
    "    start1 = time.time()\n",
    "    initialValues = spark.sparkContext.parallelize(sortedData, numPartitions).mapPartitions(lambda x: find_skyline_sfs(x, weights)).collect()\n",
    "    end = time.time() - start1\n",
    "    print(\"Length of the local skylines after parallel section is : \" + str(len(initialValues)) + \", time taken: \" + str(end))\n",
    "\n",
    "    skyline = find_skyline_sfs(initialValues, weights)\n",
    "    end2 = time.time() - start1\n",
    "    print(\"Length of the skyline is : \" + str(len(skyline)) + \", total time taken: \" + str(end2))\n",
    "\n",
    "#Angular partitioning\n",
    "def parallel_angled_partitioning_sfs(dataArray, weights, numSlices = 5):\n",
    "    \n",
    "    dimensions = len(dataArray[0])\n",
    "\n",
    "    #numberOfSlices = min(max(8,  ceil((sys.getsizeof(dataArray)/1024/1000) * 0.4 ) ), 24)\n",
    "    start = time.time()\n",
    "    # Partition By divides the dataset by the primary key of each tuple\n",
    "    initialResult = spark.sparkContext.parallelize(dataArray) \\\n",
    "                    .map(lambda x : ( get_angular_partitionIndex(x, dimensions, numSlices)  , x) )  \\\n",
    "                    .partitionBy(numSlices**(dimensions-1)) \\\n",
    "                    .mapPartitions(lambda x : execute_sfs_indexed(x, weights), preservesPartitioning=True)  \\\n",
    "                    .collect()\n",
    "    end = time.time()- start\n",
    "    print('AP: Length of skyline after parallel phase is :' + str(len(initialResult))+ \", time taken: \"+ str(end))\n",
    "    seq_time = time.time()\n",
    "    finRes = find_skyline_sfs(initialResult,weights)\n",
    "    end_seq = time.time() - seq_time\n",
    "\n",
    "    print('AP: Length of the skyline is :' + str(len(finRes))+ ', total time: ' + str(end+end_seq))\n",
    "\n",
    "def angular_sfs(dataset, weights, dimensions, numSlices = 5):\n",
    "\n",
    "    #dimensions = len(dataArray[0])\n",
    "    rdd = spark.sparkContext.textFile(os.getcwd() + '/datasets/' + dataset)\n",
    "    #numberOfSlices = min(max(8,  ceil((sys.getsizeof(dataArray)/1024/1000) * 0.4 ) ), 24)\n",
    "    start = time.time()\n",
    "    # Partition By divides the dataset by the primary key of each tuple\n",
    "    initialResult = rdd.map(lambda x : get_angular_partitionIndex2(x, dimensions, numSlices)) \\\n",
    "                    .partitionBy(numSlices**(dimensions-1)) \\\n",
    "                    .mapPartitions(lambda x : execute_sfs_indexed(x, weights), preservesPartitioning=True)  \\\n",
    "                    .collect()\n",
    "    end = time.time()- start\n",
    "    print('Length of skyline after parallel phase of Angular Partitioning is :' + str(len(initialResult))+ \", time taken: \"+ str(end))\n",
    "    seq_time = time.time()\n",
    "    finRes = find_skyline_sfs(initialResult,weights)\n",
    "    end_seq = time.time() - seq_time\n",
    "\n",
    "    print('Length of the skyline is :' + str(len(finRes))+ ', total time: ' + str(end+end_seq))\n",
    "\n",
    "\n",
    "#grid partitioning\n",
    "def parallel_grid_partitioning_sfs(dataArray, weights, numSlices = 4):\n",
    "    dimensions = len(dataArray[0])\n",
    "\n",
    "    start = time.time()\n",
    "    initialResults = spark.sparkContext.parallelize(dataArray) \\\n",
    "                    .map(lambda x : ( get_grid_partition_index(x, numSlices), x ) )  \\\n",
    "                    .partitionBy(numSlices**dimensions) \\\n",
    "                    .mapPartitions(lambda x : execute_sfs_indexed(x, weights), preservesPartitioning=True) \\\n",
    "                    .collect()\n",
    "    end = time.time()- start\n",
    "    print('Length of skyline after parallel phase of Grid Partitioning is :' + str(len(initialResults)) + \",time taken: \"+ str(end))\n",
    "    seq_time = time.time()\n",
    "    finRes = find_skyline_sfs(initialResults,weights)\n",
    "    end_seq = time.time() - seq_time\n",
    "\n",
    "    print('Length of the skyline is :' + str(len(finRes))+ ', total time: ' + str(end+end_seq))\n",
    "\n",
    "\n",
    "    \n",
    "# Angular partitioning with a serial grid filtering phase \n",
    "def sfs_angular_partitioning_with_serial_grid_filtering(datapoints, weights, numberOfSlices = 5):\n",
    "    start = time.time()\n",
    "    if numberOfSlices == -1 :\n",
    "        numberOfSlices = min(max(8,  ceil((sys.getsizeof(datapoints)/1024/1000) * 0.4 ) ), 24)\n",
    "\n",
    "    input_list = query_containers(datapoints)\n",
    "    end = time.time() - start\n",
    "    print('Time for the container serial query: ' + str(end))\n",
    "    start_parallel = time.time()\n",
    "    finalResult = parallel_angled_partitioning_sfs(input_list, weights, numberOfSlices)\n",
    "    end_parallel = time.time() - start_parallel\n",
    "    print('Total time: '+ str(end_parallel+end))\n",
    "\n",
    "def sfs_angular_partitioning_with_serial_grid_filtering2(dataset, weights, numberOfSlices = 4):\n",
    "    start = time.time()\n",
    "\n",
    "\n",
    "    input_list = query_containers(dataset, numberOfSlices)\n",
    "    end = time.time() - start\n",
    "    print('Time for the container serial query: ' + str(end))\n",
    "    start_parallel = time.time()\n",
    "    finalResult = parallel_angled_partitioning_sfs(input_list, weights)\n",
    "    end_parallel = time.time() - start_parallel\n",
    "    print('Total time: '+ str(end_parallel+end))\n",
    "    \n",
    "    \n",
    "# Grid partitioning with a serial grid filtering phase \n",
    "def sfs_grid_partitioning_with_serial_grid_filtering(datapoints, weights, numberOfSlices = 4):\n",
    "\n",
    "    start = time.time()\n",
    "    input_list = query_containers(datapoints, numberOfSlices)\n",
    "    end = time.time() - start\n",
    "    print('Time for the container serial query: ' + str(end))\n",
    "    start_parallel = time.time()\n",
    "    finalResult = parallel_grid_partitioning_sfs(input_list, weights)\n",
    "    end_parallel = time.time() - start_parallel\n",
    "    print('Total time: '+ str(end_parallel+end))\n",
    "    \n",
    "\n",
    "#Parallel3P SFS\n",
    "def AllParallel_sfs(datapoints, weights, numPartFirst=100, numPartSecond=100, numReps = 30):\n",
    "\n",
    "    start = time.time()\n",
    "    parallel_skyline = parallel_3P(datapoints, weights, numReps, numPartFirst)\n",
    "    end = time.time()-start\n",
    "    start_serial = time.time()\n",
    "    #numberOfSlices = min(max(8,  ceil((sys.getsizeof(parallel_skyline)/1024/1000) * 0.4 ) ), 24)\n",
    "    parallel_global_skyline = spark.sparkContext.parallelize(parallel_skyline, numPartSecond)\\\n",
    "                                .mapPartitions(lambda x : sfs_multithread(x, parallel_skyline, weights)) \\\n",
    "                                .collect()    \n",
    "\n",
    "    end_serial = time.time() - start_serial\n",
    "    print('Length of the skyline: ' + str(len(parallel_global_skyline)) + ', Time taken to find: ' + str(end_serial))\n",
    "    print('Total time taken with representatives: ' + str(end_serial+end))\n",
    "\n",
    "def AllParallel_sfs2(dataset, weights, numberOfSlices=30):\n",
    "\n",
    "    start = time.time()\n",
    "    parallel_skyline = parallel_representative_filtering(dataset, weights, onlyFilter = False)\n",
    "    end = time.time()-start\n",
    "    start_serial = time.time()\n",
    "    parallel_skyline.sort(key = lambda x: sort_function(x, weights))\n",
    "    #numberOfSlices = min(max(8,  ceil((sys.getsizeof(parallel_skyline)/1024/1000) * 0.4 ) ), 24)\n",
    "    parallel_global_skyline = spark.sparkContext.parallelize(parallel_skyline, numberOfSlices)\\\n",
    "                                .mapPartitions(lambda x : sfs_multithread(x, parallel_skyline, weights)) \\\n",
    "                                .collect()\n",
    "\n",
    "    end_serial = time.time() - start_serial\n",
    "    print('Length of the skyline: ' + str(len(parallel_global_skyline)) + ', Time taken to find: ' + str(end_serial))\n",
    "    print('Total time taken with representatives: ' + str(end_serial+end))\n",
    "    \n",
    "def AllParallel_sfs3(dataset, weights, numberOfSlices=30, numRep = 100):\n",
    "\n",
    "    start = time.time()\n",
    "    parallel_skyline = representative_smallest(dataset, weights, numRep, onlyFilter = False)\n",
    "    end = time.time()-start\n",
    "    start_serial = time.time()\n",
    "    #numberOfSlices = min(max(8,  ceil((sys.getsizeof(parallel_skyline)/1024/1000) * 0.4 ) ), 24)\n",
    "    parallel_global_skyline = spark.sparkContext.parallelize(parallel_skyline, numberOfSlices)\\\n",
    "                                .mapPartitions(lambda x : sfs_multithread(x, parallel_skyline, weights)) \\\n",
    "                                .collect()\n",
    "\n",
    "    end_serial = time.time() - start_serial\n",
    "    print('Length of the skyline: ' + str(len(parallel_global_skyline)) + ', Time taken to find: ' + str(end_serial))\n",
    "    print('Total time taken with representatives: ' + str(end_serial+end))\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b4ea42",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('SaLSa distributed version')\n",
    "\n",
    "#Random partitioning\n",
    "def parallel_SaLSa(data, numOfSlices =30):\n",
    "    start1 = time.time()\n",
    "\n",
    "    initialValues = spark.sparkContext.parallelize(data, numOfSlices).mapPartitions(find_skyline_SaLSa).collect()\n",
    "    end = time.time() - start1\n",
    "    print(\"Length after parallel phase : \" + str(len(initialValues)) + \", time taken: \" + str(end))\n",
    "\n",
    "    skyline = find_skyline_SaLSa(initialValues)\n",
    "    end2 = time.time() - start1\n",
    "    print(\"Length of the skyline is : \" + str(len(skyline)) + \", total time taken: \" + str(end2))\n",
    "\n",
    "def parallel_angled_partitioning_saLSa(dataArray, numSlices = 2):\n",
    "    dimensions = len(dataArray[0])\n",
    "    \n",
    "    start = time.time()\n",
    "    # Partition By divides the dataset by the primary key of each tuple\n",
    "    initialResult = spark.sparkContext.parallelize(dataArray, numSlices) \\\n",
    "                    .map(lambda x : ( get_angular_partitionIndex(x, dimensions, numSlices)  , x) )  \\\n",
    "                    .partitionBy(numSlices**(dimensions-1)) \\\n",
    "                    .mapPartitions(execute_saLSa_indexed, preservesPartitioning=True)  \\\n",
    "                    .collect()\n",
    "    end = time.time()- start\n",
    "    print('AP: Length of skyline after parallel phase is :' + str(len(initialResult))+ \", time taken: \"+ str(end))\n",
    "    seq_time = time.time()\n",
    "    finRes = find_skyline_SaLSa(initialResult)\n",
    "    end_seq = time.time() - seq_time\n",
    "\n",
    "    print('AP: Length of the skyline is :' + str(len(finRes)) + ', total time taken: '+str(end+end_seq) )\n",
    "    \n",
    "def naive_grid_partitioning_saLSa(dataArray, numSlices = 2):\n",
    "    dimensions = len(dataArray[0])\n",
    "    \n",
    "    print('Number of slices := ' + str(numSlices**dimensions))\n",
    "    \n",
    "    start = time.time()\n",
    "    m2 = spark.sparkContext.parallelize(dataArray, numSlices) \\\n",
    "                    .map(lambda x : ( get_grid_partition_index(x, numSlices), x ) )  \\\n",
    "                    .partitionBy(numSlices**dimensions) \\\n",
    "                    .mapPartitions(execute_saLSa_indexed, preservesPartitioning=True) \\\n",
    "                    .collect()\n",
    "    end = time.time()- start\n",
    "    print('GP: Length of skyline after parallel phase is :' + str(len(m2)) + \",time taken: \"+ str(end))\n",
    "    seq_time = time.time()\n",
    "    finRes = find_skyline_SaLSa(m2)\n",
    "    end_seq = time.time() - seq_time\n",
    "\n",
    "    print('AP: Length of the skyline is :' + str(len(finRes)) + ', total time taken: '+str(end+end_seq))\n",
    "    \n",
    "def saLSa_angular_partitioning_with_serial_grid_filtering(datapoints, numSlicesPerDimension = 12):\n",
    "    start = time.time()\n",
    "    input_list = query_containers(datapoints, numSlicesPerDimension)\n",
    "    end = time.time() - start\n",
    "    print(str(end) + ' for the container serial query')\n",
    "    start_parallel = time.time()\n",
    "    #finalResult = parallel_sfs(input_list, weights)\n",
    "    finalResult = naive_grid_partitioning_saLSa(input_list)\n",
    "    end_parallel = time.time() - start_parallel\n",
    "    print('Total time: '+ str(end_parallel+end))\n",
    "\n",
    "def saLSa_grid_partitioning_with_serial_grid_filtering(datapoints, numSlicesPerDimension = 12):\n",
    "    start = time.time()\n",
    "    input_list = query_containers(datapoints, numSlicesPerDimension)\n",
    "    end = time.time() - start\n",
    "    print(str(end) + ' for the container serial query')\n",
    "    start_parallel = time.time()\n",
    "    #finalResult = parallel_sfs(input_list, weights)\n",
    "    finalResult = parallel_angled_partitioning_saLSa(input_list)\n",
    "    end_parallel = time.time() - start_parallel\n",
    "    print('Total time: '+ str(end_parallel+end))\n",
    "    \n",
    "\n",
    "def parallel3P_salsa(datapoints, weights, numSlices=12):\n",
    "    start = time.time()\n",
    "    parallel_skyline = parallel_3P(datapoints, weights, numSlices)\n",
    "    end = time.time()-start\n",
    "    start_serial = time.time()\n",
    "    parallel_global_skyline = spark.sparkContext.parallelize(parallel_skyline, numSlices)\\\n",
    "                                .mapPartitions(lambda x : salsa_multithread(x, parallel_skyline)) \\\n",
    "                                .collect()\n",
    "\n",
    "    end_serial = time.time() - start_serial\n",
    "    print('Length of the skyline: ' + str(len(parallel_global_skyline)) + ', Time taken to find: ' + str(end_serial))\n",
    "    print('Total time taken with representatives: ' + str(end_serial+end))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2526fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('SVE1 distributed computation')\n",
    "#Random partitioning\n",
    "def parallelSVE1(data, vertices, p, numOfSlices = -1):\n",
    "    start = time.time()\n",
    "    if numOfSlices == -1 :\n",
    "        numOfSlices = min(max(8,  ceil((sys.getsizeof(data)/1024/1000) * 0.4 ) ), 24)\n",
    "    initialValues = spark.sparkContext.parallelize(data,numOfSlices).mapPartitions(lambda x: sve1(x, vertices, p),preservesPartitioning = True).collect()\n",
    "    \n",
    "    end = time.time()-start\n",
    "    print(\"Length of nd after parallel section : \" + str(len(initialValues)) + \", time taken: \" + str(end))\n",
    "    start2 = time.time()\n",
    "    nd = sve1(initialValues,vertices, p)\n",
    "    end2= time.time()-start2\n",
    "    print(\"Length of nd is : \" + str(len(nd)) + \", time taken: \" + str(end2))\n",
    "    print(\"Total time nd: \" +str(end+end2))\n",
    "    \n",
    "    return nd\n",
    "\n",
    "def parallel_angled_partitioning_sve1(dataArray, vertices, p, numSlices = 2):\n",
    "    \n",
    "    dimensions = len(dataArray[0])\n",
    "\n",
    "    numOfSlices = min(max(8,  ceil((sys.getsizeof(dataArray)/1024/1000) * 0.4 ) ), 24)\n",
    "    start = time.time()\n",
    "    # Partition By divides the dataset by the primary key of each tuple\n",
    "    initialResult = spark.sparkContext.parallelize(dataArray, numOfSlices) \\\n",
    "                    .map(lambda x : ( get_angular_partitionIndex(x, dimensions, numSlices)  , x) )  \\\n",
    "                    .partitionBy(numSlices**(dimensions-1)) \\\n",
    "                    .mapPartitions(lambda x : execute_sve1_indexed(x, vertices, p), preservesPartitioning=True)  \\\n",
    "                    .collect()\n",
    "    end = time.time()- start\n",
    "    print('AP: Length of nd after parallel phase is :' + str(len(initialResult))+ \", time taken: \"+ str(end))\n",
    "    seq_time = time.time()\n",
    "    finRes = sve1(initialResult,vertices, p)\n",
    "    end_seq = time.time() - seq_time\n",
    "\n",
    "    print('AP: Length of nd is :' + str(len(finRes)) + ', total time taken: '+str(end+end_seq) )\n",
    "    return finRes\n",
    "\n",
    "def naive_grid_partitioning_sve1(dataArray, vertices, p, numSlices = 2):\n",
    "    dimensions = len(dataArray[0])\n",
    "    \n",
    "    print('Number of slices := ' + str(numSlices**dimensions))\n",
    "\n",
    "    numOfSlices = min(max(8,  ceil((sys.getsizeof(data)/1024/1000) * 0.4 ) ), 24)\n",
    "    start = time.time()\n",
    "    initialResult = spark.sparkContext.parallelize(dataArray, numOfSlices) \\\n",
    "                    .map(lambda x : ( get_grid_partition_index(x, numSlices), x ) )  \\\n",
    "                    .partitionBy(numSlices**dimensions) \\\n",
    "                    .mapPartitions(lambda x : execute_sve1_indexed(x, vertices, p), preservesPartitioning=True) \\\n",
    "                    .collect()\n",
    "    end = time.time()- start\n",
    "    print('GP: Length of nd after parallel phase is :' + str(len(initialResult))+ \", time taken: \"+ str(end))\n",
    "\n",
    "    seq_time = time.time()\n",
    "    finRes = sve1(initialResult,vertices,p)\n",
    "    end_seq = time.time() - seq_time\n",
    "\n",
    "    print('GP: Length of nd is :' + str(len(finRes)) + ', total time taken: '+str(end+end_seq) )\n",
    "    return finRes\n",
    "\n",
    "def sve1_angular_partitioning_with_serial_grid_filtering(datapoints, vertices, p, numOfSlices = -1):\n",
    "    if numOfSlices == -1 :\n",
    "        numOfSlices = min(max(8,  ceil((sys.getsizeof(data)/1024/1000) * 0.4 ) ), 24)\n",
    "    start = time.time()\n",
    "    input_list = query_containers(datapoints, numOfSlices)\n",
    "    end = time.time() - start\n",
    "    print(str(end) + ' for the container serial query')\n",
    "    start_parallel = time.time()\n",
    "    finalResult = parallel_angled_partitioning_sve1(input_list, vertices, p)\n",
    "    end_parallel = time.time() - start_parallel\n",
    "    print('Total time: '+ str(end_parallel+end))\n",
    "    \n",
    "def sve1_grid_partitioning_with_serial_grid_filtering(datapoints, vertices, p, numOfSlices = -1):\n",
    "    if numOfSlices == -1 :\n",
    "        numOfSlices = min(max(8,  ceil((sys.getsizeof(data)/1024/1000) * 0.4 ) ), 24)\n",
    "    start = time.time()\n",
    "    input_list = query_containers(datapoints, numOfSlices)\n",
    "    end = time.time() - start\n",
    "    print(str(end) + ' for the container serial query')\n",
    "    start_parallel = time.time()\n",
    "    finalResult = naive_grid_partitioning_sve1(input_list, vertices, p)\n",
    "    end_parallel = time.time() - start_parallel\n",
    "    print('Total time: '+ str(end_parallel+end))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "607113e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('SVE1F distributed computation')\n",
    "import random\n",
    "#Random partitioning\n",
    "def parallelSVE1F(data, vertices, p, numOfSlices = 100):\n",
    "\n",
    "    start = time.time()\n",
    "    initialValues = spark.sparkContext.parallelize(data,numOfSlices).mapPartitions(lambda x: sve1f(x, vertices, p),\n",
    "                                                             preservesPartitioning = True).collect()\n",
    "    end = time.time()-start\n",
    "    print(\"Length of nd after parallel section: \" + str(len(initialValues)) + \", time taken: \" + str(end))\n",
    "    start_seq = time.time()\n",
    "    nd = sve1f(initialValues, vertices, p)\n",
    "    end_seq= time.time()-start_seq\n",
    "    print(\"Length of nd is : \" + str(len(nd)) + \", time taken: \" + str(end_seq))\n",
    "    print(\"Total time nd: \" +str(end+end_seq))\n",
    "    \n",
    "    return nd\n",
    "\n",
    "#One-slice partitioning\n",
    "\n",
    "def sliced_partitioning_sve1f(datapoints, vertices, p, numPartitions=100):\n",
    "    start = time.time()\n",
    "    datapoints.sort(key = lambda x : x[0])\n",
    "    sortedData = datapoints\n",
    "    end = time.time() - start\n",
    "    print('Time taken for sorting: '+ str(end))\n",
    "\n",
    "    start1 = time.time()\n",
    "    initialValues = spark.sparkContext.parallelize(sortedData, numPartitions).mapPartitions(lambda x: sve1f(x, vertices, p)).collect()\n",
    "    end = time.time() - start1\n",
    "    print(\"Length of the local skylines after parallel section is : \" + str(len(initialValues)) + \", time taken: \" + str(end))\n",
    "\n",
    "    nd = sve1f(initialValues, vertices, p)\n",
    "    end2 = time.time() - start1\n",
    "    print(\"Length of nd is : \" + str(len(nd)) + \", total time taken: \" + str(end2))\n",
    "\n",
    "def parallel_angled_partitioning_sve1f(dataArray, vertices, p, numSlices = 5):\n",
    "    \n",
    "    dimensions = len(dataArray[0])\n",
    "    #numOfSlices = min(max(8,  ceil((sys.getsizeof(dataArray)/1024/1000) * 0.4 ) ), 24)\n",
    "    start = time.time()\n",
    "    # Partition By divides the dataset by the primary key of each tuple\n",
    "    initialResult = spark.sparkContext.parallelize(dataArray) \\\n",
    "                    .map(lambda x : ( get_angular_partitionIndex(x, dimensions, numSlices)  , x) )  \\\n",
    "                    .partitionBy(numSlices**(dimensions-1)) \\\n",
    "                    .mapPartitions(lambda x : execute_sve1f_indexed(x, vertices, p))  \\\n",
    "                    .collect()\n",
    "    end = time.time()- start\n",
    "    print('AP: Length of nd after parallel phase is :' + str(len(initialResult))+ \", time taken: \"+ str(end))\n",
    "    seq_time = time.time()\n",
    "    finRes = sve1f(initialResult,vertices, p)\n",
    "    end_seq = time.time() - seq_time\n",
    "\n",
    "    print('AP: Length of the skyline is :' + str(len(finRes)) + ', total time taken: '+str(end+end_seq))\n",
    "    return finRes\n",
    "    \n",
    "def naive_grid_partitioning_sve1f(dataArray, vertices, p, numSlices = 4):\n",
    "    dimensions = len(dataArray[0])\n",
    "\n",
    "\n",
    "    print('Number of slices := ' + str(numSlices**dimensions))\n",
    "    \n",
    "    start = time.time()\n",
    "    initialResult = spark.sparkContext.parallelize(dataArray) \\\n",
    "                    .map(lambda x : ( get_grid_partition_index(x, numSlices), x ) )  \\\n",
    "                    .partitionBy(numSlices**dimensions) \\\n",
    "                    .mapPartitions(lambda x : execute_sve1f_indexed(x, vertices, p)) \\\n",
    "                    .collect()\n",
    "    end = time.time()- start\n",
    "    print('GP: Length of nd after parallel phase is :' + str(len(initialResult))+ \", time taken: \"+ str(end))\n",
    "\n",
    "    seq_time = time.time()\n",
    "    finRes = sve1f(initialResult,vertices,p)\n",
    "    end_seq = time.time() - seq_time\n",
    "\n",
    "    print('GP: Length of the skyline is :' + str(len(finRes)) + ', total time taken: '+str(end+end_seq))\n",
    "    return finRes\n",
    "\n",
    "def sve1f_grid_partitioning_with_serial_grid_filtering(datapoints, vertices, p, numOfSlices = -1):\n",
    "    if numOfSlices == -1 :\n",
    "        numOfSlices = min(max(8,  ceil((sys.getsizeof(datapoints)/1024/1000) * 0.4 ) ), 24)\n",
    "    start = time.time()\n",
    "    input_list = query_containers(datapoints, numOfSlices)\n",
    "    end = time.time() - start\n",
    "    print(str(end) + ' for the container serial query')\n",
    "    start_parallel = time.time()\n",
    "    #finalResult = parallel_sfs(input_list, weights)\n",
    "    finalResult = naive_grid_partitioning_sve1f(input_list, vertices, p)\n",
    "    end_parallel = time.time() - start_parallel\n",
    "    print('Total time: '+ str(end_parallel+end))\n",
    "\n",
    "def sve1f_angular_partitioning_with_serial_grid_filtering(datapoints, vertices, p):\n",
    "\n",
    "    start = time.time()\n",
    "    input_list = query_containers(datapoints)\n",
    "    end = time.time() - start\n",
    "    print(str(end) + ' for the container serial query')\n",
    "    start_parallel = time.time()\n",
    "    #finalResult = parallel_sfs(input_list, weights)\n",
    "    finalResult = parallel_angled_partitioning_sve1f(input_list, vertices, p)\n",
    "    end_parallel = time.time() - start_parallel\n",
    "    print('Total time: '+ str(end_parallel+end))\n",
    "\n",
    "\n",
    "def allParallel_sve1f(datapoints, vertices, p, numOfSlices=30):\n",
    "\n",
    "    start = time.time()\n",
    "    centroids = compute_centroid(vertices)\n",
    "    parallel_skyline = parallel_3P_sve1f(datapoints, vertices, p, numOfSlices)\n",
    "    end = time.time()-start\n",
    "    start_serial = time.time()\n",
    "    parallel_global_skyline = spark.sparkContext.parallelize(parallel_skyline, numOfSlices*2)\\\n",
    "                                .mapPartitions(lambda x : sve1f_multithread(x, parallel_skyline, vertices, p)) \\\n",
    "                                .collect()\n",
    "\n",
    "    end_serial = time.time() - start_serial\n",
    "    print('Length of nd: ' + str(len(parallel_global_skyline)) + ', Time taken to find: ' + str(end_serial))\n",
    "    print('Total time taken with representatives: ' + str(end_serial+end))\n",
    "    return parallel_global_skyline\n",
    "\n",
    "def allParallel_sve1f2(datapoints, vertices, p, numOfSlices=20):\n",
    "\n",
    "    start = time.time()\n",
    "    parallel_skyline = parallel_representative_filtering_nd(datapoints, vertices, p, numOfSlices, onlyFilter=False)\n",
    "    end = time.time()-start\n",
    "    start_serial = time.time()\n",
    "    dimensions = len(datapoints[0])\n",
    "    datapoints.sort(key=lambda x: x[0])\n",
    "    parallel_global_skyline = spark.sparkContext.parallelize(parallel_skyline, numOfSlices)\\\n",
    "                                .mapPartitions(lambda x : sve1f_multithread(x, parallel_skyline, vertices, p)) \\\n",
    "                                .collect()\n",
    "\n",
    "    end_serial = time.time() - start_serial\n",
    "    print('Length of nd: ' + str(len(parallel_global_skyline)) + ', Time taken to find: ' + str(end_serial))\n",
    "    print('Total time taken with representatives: ' + str(end_serial+end))\n",
    "    return parallel_global_skyline\n",
    "\n",
    "def allParallel_sve1f3(datapoints, vertices, p, numOfSlices=200):\n",
    "\n",
    "    start = time.time()\n",
    "    centr = compute_centroid(vertices)\n",
    "    parallel_skyline = parallel_representative_filtering(datapoints, centr,numOfSlices, onlyFilter=False)\n",
    "    end = time.time()-start\n",
    "    start_serial = time.time()\n",
    "    dimensions = len(datapoints[0])\n",
    "    datapoints.sort(key=lambda x: x[0])\n",
    "    parallel_global_skyline = spark.sparkContext.parallelize(parallel_skyline, numOfSlices)\\\n",
    "                                .mapPartitions(lambda x : sve1f_multithread(x, parallel_skyline, vertices, p)) \\\n",
    "                                .collect()\n",
    "\n",
    "    end_serial = time.time() - start_serial\n",
    "    print('Length of nd: ' + str(len(parallel_global_skyline)) + ', Time taken to find: ' + str(end_serial))\n",
    "    print('Total time taken with representatives: ' + str(end_serial+end))\n",
    "    return parallel_global_skyline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49234df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('PO primal gurobipy distributed computation')\n",
    "#Random partitioning\n",
    "def parallelPOND_primal(nd, constraints, k_, numSlices = 60):\n",
    "    start = time.time()\n",
    "    initialValues = spark.sparkContext.parallelize(nd, numSlices).mapPartitions(lambda x: po_primal(x, constraints, k_), preservesPartitioning = True).collect()\n",
    "    end = time.time()-start\n",
    "    print(\"Length of po after parallel section: \" + str(len(initialValues)) + \", time taken: \" + str(end))\n",
    "    start_seq = time.time()\n",
    "    po = po_primal(initialValues, constraints, k_)\n",
    "    end_seq= time.time()-start_seq\n",
    "    print(\"Length of po is : \" + str(len(po)) + \", time taken: \" + str(end_seq))\n",
    "    print(\"Total time po: \" +str(end+end_seq+end_nd))\n",
    "\n",
    "def parallel_angled_partitioning_pond_primal(nd, constraints, k_, numSlices = 5):\n",
    "    \n",
    "    dimensions = len(nd[0])\n",
    "    \n",
    "    start = time.time()\n",
    "    # Partition By divides the dataset by the primary key of each tuple\n",
    "    initialResult = spark.sparkContext.parallelize(nd, numSlices**(dimensions-1)) \\\n",
    "                    .map(lambda x : ( get_angular_partitionIndex(x, dimensions, numSlices)  , x) )  \\\n",
    "                    .partitionBy(numSlices**(dimensions-1)) \\\n",
    "                    .mapPartitions(lambda x : execute_pond_indexed_primal(x, constraints, k_), preservesPartitioning=True)  \\\n",
    "                    .collect()\n",
    "    end = time.time()- start\n",
    "    print('AP: Length of po after parallel phase is :' + str(len(initialResult))+ \", time taken: \"+ str(end))\n",
    "    seq_time = time.time()\n",
    "    finRes = po_primal(initialResult,constraints, k_)\n",
    "    end_seq = time.time() - seq_time\n",
    "\n",
    "    print('AP: Length of po is :' + str(len(finRes)) + ', total time taken: '+str(end+end_seq))\n",
    "    return finRes\n",
    "\n",
    "def naive_grid_partitioning_pond_primal(nd, constraints, k_, numSlices = 6):\n",
    "    dimensions = len(nd[0])\n",
    "    \n",
    "    print('Number of slices := ' + str(numSlices**dimensions))\n",
    "    \n",
    "    start = time.time()\n",
    "    initialResult = spark.sparkContext.parallelize(nd, numSlices) \\\n",
    "                    .map(lambda x : ( get_grid_partition_index(x, numSlices), x ) )  \\\n",
    "                    .partitionBy(numSlices**dimensions) \\\n",
    "                    .mapPartitions(lambda x : execute_pond_indexed_primal(x, constraints, k_), preservesPartitioning=True) \\\n",
    "                    .collect()\n",
    "    end = time.time()- start\n",
    "    print('GP: Length of po after parallel phase is :' + str(len(initialResult))+ \", time taken: \"+ str(end))\n",
    "\n",
    "    seq_time = time.time()\n",
    "    finRes = po_primal(initialResult,constraints, k_)\n",
    "    end_seq = time.time() - seq_time\n",
    "\n",
    "    print('GP: Length of the skyline is :' + str(len(finRes)) + ', total time taken: '+str(end+end_seq))\n",
    "    return finRes\n",
    "    \n",
    "def pond_primal_angled_partitioning_with_serial_grid_filtering(datapoints, constraints, k_, numSlicesPerDimension = 12):\n",
    "    start = time.time()\n",
    "    input_list = query_containers(datapoints, numSlicesPerDimension)\n",
    "    end = time.time() - start\n",
    "    print(str(end) + ' for the container serial query')\n",
    "    start_parallel = time.time()\n",
    "    finalResult = naive_grid_partitioning_pond_primal(input_list, constraints, k_)\n",
    "    end_parallel = time.time() - start_parallel\n",
    "    print('Total time: '+ str(end_parallel+end))\n",
    "\n",
    "def pond_primal_grid_partitioning_with_serial_grid_filtering(datapoints, constraints, k_, numSlicesPerDimension = 12):\n",
    "    start = time.time()\n",
    "    input_list = query_containers(datapoints, numSlicesPerDimension)\n",
    "    end = time.time() - start\n",
    "    print(str(end) + ' for the container serial query')\n",
    "    start_parallel = time.time()\n",
    "    finalResult = naive_grid_partitioning_pond_primal(input_list, constraints, k_)\n",
    "    end_parallel = time.time() - start_parallel\n",
    "    print('Total time: '+ str(end_parallel+end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc3c8ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('PO primal pulp distributed computation')\n",
    "#Random partitioning\n",
    "import random\n",
    "def parallelPOND_primal_pulp(nd, constraints, k_, numSlices = 60):\n",
    "    start = time.time()\n",
    "    random.shuffle(nd)\n",
    "    initialValues = spark.sparkContext.parallelize(nd, numSlices).mapPartitions(lambda x: po_primal_pulp(x, constraints, k_)).collect()\n",
    "    end = time.time()-start\n",
    "    print(\"Length of po after parallel section: \" + str(len(initialValues)) + \", time taken: \" + str(end))\n",
    "    start_seq = time.time()\n",
    "    po = po_primal(initialValues, constraints, k_)\n",
    "    end_seq= time.time()-start_seq\n",
    "    print(\"Length of po is : \" + str(len(po)) + \", time taken: \" + str(end_seq))\n",
    "    print(\"Total time po: \" +str(end+end_seq))\n",
    "\n",
    "def parallel_angled_partitioning_pond_primal_pulp(nd, constraints, k_, numSlices = 5):\n",
    "\n",
    "    dimensions = len(nd[0])\n",
    "\n",
    "    start = time.time()\n",
    "    # Partition By divides the dataset by the primary key of each tuple\n",
    "    initialResult = spark.sparkContext.parallelize(nd, numSlices**(dimensions-1)) \\\n",
    "                    .map(lambda x : ( get_angular_partitionIndex(x, dimensions, numSlices)  , x) )  \\\n",
    "                    .partitionBy(numSlices**(dimensions-1)) \\\n",
    "                    .mapPartitions(lambda x : execute_pond_indexed_primal_pulp(x, constraints, k_), preservesPartitioning=True)  \\\n",
    "                    .collect()\n",
    "    end = time.time()- start\n",
    "    print('AP: Length of po after parallel phase is :' + str(len(initialResult))+ \", time taken: \"+ str(end))\n",
    "    seq_time = time.time()\n",
    "    finRes = po_primal(initialResult,constraints, k_)\n",
    "    end_seq = time.time() - seq_time\n",
    "\n",
    "    print('AP: Length of po is :' + str(len(finRes)) + ', total time taken: '+str(end+end_seq))\n",
    "    return finRes\n",
    "\n",
    "def naive_grid_partitioning_pond_primal_pulp(nd, constraints, k_, numSlices = 6):\n",
    "    dimensions = len(nd[0])\n",
    "\n",
    "    print('Number of slices := ' + str(numSlices**dimensions))\n",
    "\n",
    "    start = time.time()\n",
    "    initialResult = spark.sparkContext.parallelize(nd, numSlices) \\\n",
    "                    .map(lambda x : ( get_grid_partition_index(x, numSlices), x ) )  \\\n",
    "                    .partitionBy(numSlices**dimensions) \\\n",
    "                    .mapPartitions(lambda x : execute_pond_indexed_primal_pulp(x, constraints, k_), preservesPartitioning=True) \\\n",
    "                    .collect()\n",
    "    end = time.time()- start\n",
    "    print('GP: Length of po after parallel phase is :' + str(len(initialResult))+ \", time taken: \"+ str(end))\n",
    "\n",
    "    seq_time = time.time()\n",
    "    finRes = po_primal(initialResult,constraints, k_)\n",
    "    end_seq = time.time() - seq_time\n",
    "\n",
    "    print('GP: Length of the skyline is :' + str(len(finRes)) + ', total time taken: '+str(end+end_seq))\n",
    "    return finRes\n",
    "\n",
    "#One-slice partitioning\n",
    "\n",
    "def sliced_partitioning_po_primal_pulp(datapoints, contraints, k_, numPartitions=100):\n",
    "    start = time.time()\n",
    "    datapoints.sort(key = lambda x : x[0])\n",
    "    sortedData = datapoints\n",
    "    end = time.time() - start\n",
    "    print('Time taken for sorting: '+ str(end))\n",
    "\n",
    "    start1 = time.time()\n",
    "    initialValues = spark.sparkContext.parallelize(sortedData, numPartitions).mapPartitions(lambda x: po_primal_pulp(x, constraints, k_)).collect()\n",
    "    end = time.time() - start1\n",
    "    print(\"Length of the local po set after parallel section is : \" + str(len(initialValues)) + \", time taken: \" + str(end))\n",
    "\n",
    "    nd = po_primal(initialValues, constraints, k_)\n",
    "    end2 = time.time() - start1\n",
    "    print(\"Length of PO is : \" + str(len(nd)) + \", total time taken: \" + str(end2))\n",
    "\n",
    "def allParallel_pond_primal(nd, constraints, k_, numOfSlices=-1):\n",
    "    if numOfSlices == -1 :\n",
    "        numOfSlices = min(max(8,  ceil((sys.getsizeof(datapoints)/1024/1000) * 0.4 ) ), 24)\n",
    "    start_parallel = time.time()\n",
    "    dimensions = len(nd[0])\n",
    "    random.shuffle(nd)\n",
    "    initialResult = spark.sparkContext.parallelize(nd, numOfSlices)\\\n",
    "                                .mapPartitions(lambda x : po_primal_pulp(x, constraints, k_)) \\\n",
    "                                .collect()\n",
    "    end_parallel = time.time()-start_parallel\n",
    "    print('Length of po after parallel phase: ' + str(len(initialResult)) + ', Time taken to find: ' + str(end_parallel))\n",
    "    po = spark.sparkContext.parallelize(initialResult, numOfSlices*2)\\\n",
    "                                .mapPartitions(lambda x : pond_primal_pulp_multithread(x, initialResult, constraints, k_)) \\\n",
    "                                .collect()\n",
    "    end_parallel = time.time()-start_parallel\n",
    "    print('Length of po: ' + str(len(po)) + ', Time taken to find: ' + str(end_parallel))\n",
    "\n",
    "    return po\n",
    "\n",
    "def allParallel_pond_primal_angular(nd, constraints, k_, numOfSlices=100, numSlices = 5):\n",
    "    if numOfSlices == -1 :\n",
    "        numOfSlices = min(max(8,  ceil((sys.getsizeof(datapoints)/1024/1000) * 0.4 ) ), 24)\n",
    "    start_parallel = time.time()\n",
    "    dimensions = len(nd[0])\n",
    "    initialResult = spark.sparkContext.parallelize(nd, numSlices**(dimensions-1)) \\\n",
    "                    .map(lambda x : ( get_angular_partitionIndex(x, dimensions, numSlices)  , x) )  \\\n",
    "                    .partitionBy(numSlices**(dimensions-1)) \\\n",
    "                    .mapPartitions(lambda x : execute_pond_indexed_primal_pulp(x, constraints, k_), preservesPartitioning=True)  \\\n",
    "                    .collect()\n",
    "    end_parallel = time.time()-start_parallel\n",
    "    print('Length of po after parallel phase: ' + str(len(initialResult)) + ', Time taken to find: ' + str(end_parallel))\n",
    "    po = spark.sparkContext.parallelize(initialResult, numOfSlices)\\\n",
    "                                .mapPartitions(lambda x : pond_primal_pulp_multithread(x, initialResult, constraints, k_)) \\\n",
    "                                .collect()\n",
    "    end_parallel = time.time()-start_parallel\n",
    "    print('Length of po: ' + str(len(po)) + ', Time taken to find: ' + str(end_parallel))\n",
    "\n",
    "    return po"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1dcd07",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('PO dual gurobipy distributed computation')\n",
    "#Random partitioning\n",
    "def parallelPOND_dual(nd, vertices, p, numSlices = 16):\n",
    "    start = time.time()\n",
    "    initialValues = spark.sparkContext.parallelize(nd, numSlices).mapPartitions(lambda x: po_dual(x, vertices, p), preservesPartitioning = True).collect()\n",
    "    end = time.time()-start\n",
    "    print(\"Length of po after parallel section: \" + str(len(initialValues)) + \", time taken: \" + str(end))\n",
    "    start_seq = time.time()\n",
    "    po = po_dual(initialValues, vertices, p)\n",
    "    end_seq= time.time()-start_seq\n",
    "    print(\"Length of po is : \" + str(len(po)) + \", time taken: \" + str(end_seq))\n",
    "    print(\"Total time po: \" +str(end+end_seq+end_nd))\n",
    "\n",
    "def parallel_angled_partitioning_pond_dual(nd, vertices, p, numSlices = 2):\n",
    "    \n",
    "    dimensions = len(nd[0])\n",
    "    \n",
    "    start = time.time()\n",
    "    # Partition By divides the dataset by the primary key of each tuple\n",
    "    initialResult = spark.sparkContext.parallelize(nd, numSlices) \\\n",
    "                    .map(lambda x : ( get_angular_partitionIndex(x, dimensions, numSlices)  , x) )  \\\n",
    "                    .partitionBy(numSlices**(dimensions-1)) \\\n",
    "                    .mapPartitions(lambda x : execute_pond_indexed_dual(x, vertices, p), preservesPartitioning=True)  \\\n",
    "                    .collect()\n",
    "    end = time.time()- start\n",
    "    print('AP: Length of po after parallel phase is :' + str(len(initialResult))+ \", time taken: \"+ str(end))\n",
    "    seq_time = time.time()\n",
    "    finRes = po_dual(initialResult,vertices, p)\n",
    "    end_seq = time.time() - seq_time\n",
    "\n",
    "    print('AP: Length of the skyline is :' + str(len(finRes)) + ', total time taken: '+str(end+end_seq))\n",
    "\n",
    "def naive_grid_partitioning_pond_dual(nd, vertices, p, numSlices = 2):\n",
    "    dimensions = len(nd[0])\n",
    "    \n",
    "    print('Number of slices := ' + str(numSlices**dimensions))\n",
    "    \n",
    "    start = time.time()\n",
    "    initialResult = spark.sparkContext.parallelize(nd, numSlices) \\\n",
    "                    .map(lambda x : ( get_grid_partition_index(x, numSlices), x ) )  \\\n",
    "                    .partitionBy(numSlices**dimensions) \\\n",
    "                    .mapPartitions(lambda x : execute_pond_indexed_dual(x, vertices, p), preservesPartitioning=True) \\\n",
    "                    .collect()\n",
    "    end = time.time()- start\n",
    "    print('GP: Length of po after parallel phase is :' + str(len(initialResult))+ \", time taken: \"+ str(end))\n",
    "\n",
    "    seq_time = time.time()\n",
    "    finRes = po_dual(initialResult,vertices, p)\n",
    "    end_seq = time.time() - seq_time\n",
    "\n",
    "    print('GP: Length of the skyline is :' + str(len(finRes)) + ', total time taken: '+str(end+end_seq))\n",
    "    \n",
    "def parallel_angular_partitioning_pond_dual(datapoints, vertices, p, numSlicesPerDimension = 12):\n",
    "    start = time.time()\n",
    "    input_list = query_containers(datapoints, numSlicesPerDimension)\n",
    "    end = time.time() - start\n",
    "    print(str(end) + ' for the container serial query')\n",
    "    start_parallel = time.time()\n",
    "    finalResult = parallel_angled_partitioning_pond_dual(input_list, vertices, p)\n",
    "    end_parallel = time.time() - start_parallel\n",
    "    print('Total time: '+ str(end_parallel+end))\n",
    "\n",
    "def pond_primal_grid_partitioning_with_serial_grid_filtering(datapoints, constraints, k_, numSlicesPerDimension = 12):\n",
    "    start = time.time()\n",
    "    input_list = query_containers(datapoints, numSlicesPerDimension)\n",
    "    end = time.time() - start\n",
    "    print(str(end) + ' for the container serial query')\n",
    "    start_parallel = time.time()\n",
    "    finalResult = naive_grid_partitioning_pond_primal(input_list, vertices, p)\n",
    "    end_parallel = time.time() - start_parallel\n",
    "    print('Total time: '+ str(end_parallel+end))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9127cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Sorted method and others')\n",
    "def euclidean_dist(point):\n",
    "    tot = 0\n",
    "    for p in point:\n",
    "        tot = tot + p**2\n",
    "    return tot \n",
    "def get_best_representatives_sort(index, dataset, n = 30):\n",
    "    # index is the dimension which we should check\n",
    "    # is a tuple with area_covered as the first element and the n-dimensional point as the second element\n",
    "\n",
    "    best_n_points = []\n",
    "    \n",
    "    for i in range(n):\n",
    "        best_n_points.append((0,[]))\n",
    "    counter = 0\n",
    "    for point in dataset:\n",
    "        value = euclidean_dist(point)\n",
    "        rep_index = floor(point[index]*n)\n",
    "        if best_n_points[rep_index][0] == 0 or best_n_points[rep_index][0] > value:\n",
    "            best_n_points[rep_index] = (value, point)\n",
    "    best_n_points = [x[1] for x in best_n_points if x[1]]\n",
    "    return best_n_points\n",
    "\n",
    "def representative_sorted_euclidean(datapoints, weights, numRep, numSlices=12, onlyFilter = True):\n",
    "    data_sorted = datapoints.copy()\n",
    "    data_sorted.sort(key=lambda x: euclidean_dist(x))\n",
    "    start_parallel = time.time()\n",
    "    parallel_skyline = []\n",
    "    numRep = numRep*len(datapoints[0])\n",
    "    \n",
    "    representative = find_skyline_sfs(data_sorted[0:numRep], weights)\n",
    "    #print(representative[0:10])\n",
    "    end = time.time()-start_parallel\n",
    "    print('Length of representatives: ' + str(len(representative)) + ', time taken to find: '+ str(end))\n",
    "    parallel_skyline = spark.sparkContext.parallelize(datapoints, numSlices)\\\n",
    "                                .mapPartitions(lambda x : filter_with_memory(x, representative, weights, onlyFilter)) \\\n",
    "                                .collect()\n",
    "    end_parallel = time.time() - start_parallel\n",
    "    \n",
    "    print('Time taken to filter: ' +str(end_parallel))\n",
    "    print('Length of the data after filter: ' + str(len(parallel_skyline)))\n",
    "    return parallel_skyline\n",
    "\n",
    "def representative_sorted_centroids(datapoints, weights, numRep, numSlices=12, onlyFilter = True):\n",
    "    data_sorted = datapoints.copy()\n",
    "    data_sorted.sort(key=lambda x: sort_function(x, weights))\n",
    "    start_parallel = time.time()\n",
    "    parallel_skyline = []\n",
    "    numRep = numRep*len(datapoints[0])\n",
    "\n",
    "    representative = find_skyline_sfs(data_sorted[0:numRep], weights)\n",
    "    #print(representative[0:10])\n",
    "    end = time.time()-start_parallel\n",
    "    print('Length of representatives: ' + str(len(representative)) + ', time taken to find: '+ str(end))\n",
    "    parallel_skyline = spark.sparkContext.parallelize(datapoints, numSlices)\\\n",
    "                                .mapPartitions(lambda x : filter_with_memory(x, representative, weights, onlyFilter)) \\\n",
    "                                .collect()\n",
    "    end_parallel = time.time() - start_parallel\n",
    "\n",
    "    print('Time taken to filter: ' +str(end_parallel))\n",
    "    print('Length of the data after filter: ' + str(len(parallel_skyline)))\n",
    "    return parallel_skyline\n",
    "\n",
    "def representative_sorted2(datapoints, weights, numRep, numSlices=12):\n",
    "    data_sorted = datapoints.copy()\n",
    "    data_sorted.sort(key=lambda x: x[0])\n",
    "    start_parallel = time.time()\n",
    "    parallel_skyline = []\n",
    "    #numRep = numRep*len(datapoints[0])\n",
    "    representatives = spark.sparkContext.parallelize(data_sorted, len(data_sorted[0])) \\\n",
    "                                .mapPartitionsWithIndex(lambda index, y: get_best_representatives_sort(index, y, numRep)) \\\n",
    "                                .collect()\n",
    "    representative = find_skyline_sfs(representatives, weights)\n",
    "    #print(representative[0:10])\n",
    "    end = time.time()-start_parallel\n",
    "    print('Length of representatives: ' + str(len(representative)) + ', time taken to find: '+ str(end))\n",
    "    parallel_skyline = spark.sparkContext.parallelize(datapoints, numSlices)\\\n",
    "                                .mapPartitions(lambda x : filter_with_memory(x, representative, weights, onlyFilter=True)) \\\n",
    "                                .collect()\n",
    "    end_parallel = time.time() - start_parallel\n",
    "    \n",
    "    print('Time taken to filter: ' +str(end_parallel))\n",
    "    print('Length of the data after filter: ' + str(len(parallel_skyline)))\n",
    "    return parallel_skyline\n",
    "\n",
    "def representative_sorted_nd(datapoints, vertices, p, numRep, numSlices=12, onlyFilter = True):\n",
    "    centroids = compute_centroid(vertices)\n",
    "    data_sorted = datapoints.copy()\n",
    "    data_sorted.sort(key=lambda x: sort_function(x, centroids))\n",
    "    start_parallel = time.time()\n",
    "    parallel_skyline = []\n",
    "    start = time.time()\n",
    "    numRep = numRep*len(datapoints[0])\n",
    "    representative = sve1f(data_sorted[0:numRep], vertices, p)\n",
    "    end = time.time()-start\n",
    "    print('Length of representatives: ' + str(len(representative)) + ', time taken to find: '+ str(end))\n",
    "    parallel_skyline = spark.sparkContext.parallelize(datapoints, numSlices)\\\n",
    "                                .mapPartitions(lambda x : filter_nd_with_memory2(x, representative, vertices, p, onlyFilter)) \\\n",
    "                                .collect()\n",
    "    end_parallel = time.time() - start_parallel\n",
    "    \n",
    "    print('Time taken to filter: ' +str(end_parallel))\n",
    "    print('Length of the data after filter: ' + str(len(parallel_skyline)))\n",
    "    return parallel_skyline\n",
    "\n",
    "def representative_sorted_nd_angular(datapoints, vertices, p, numRep, numSlices):\n",
    "    centroids = compute_centroid(vertices)\n",
    "    data_sorted = datapoints.copy()\n",
    "    data_sorted.sort(key=lambda x: sort_function(x, centroids))\n",
    "    start_parallel = time.time()\n",
    "    parallel_skyline = []\n",
    "    dimensions = len(datapoints[0])\n",
    "    start = time.time()\n",
    "    numRep = numRep*dimensions\n",
    "    representative = sve1f(data_sorted[0:numRep], vertices, p)\n",
    "    end = time.time()-start\n",
    "    print('Length of representatives: ' + str(len(representative)) + ', time taken to find: '+ str(end))\n",
    "    parallel_skyline = spark.sparkContext.parallelize(datapoints, numSlices**(dimensions-1)) \\\n",
    "                        .map(lambda x : ( get_angular_partitionIndex(x, dimensions, numSlices)  , x) )  \\\n",
    "                        .partitionBy(numSlices**(dimensions-1)) \\\n",
    "                        .mapPartitions(lambda x : execute_sve1_filter_with_memory(x, representative, vertices, p)) \\\n",
    "                        .collect()\n",
    "    end_parallel = time.time() - start_parallel\n",
    "    \n",
    "    print('Time taken to filter: ' +str(end_parallel))\n",
    "    print('Length of the data after filter: ' + str(len(parallel_skyline)))\n",
    "    return parallel_skyline\n",
    "\n",
    "def get_angular_representatives(dataset, n, weights):\n",
    "    if not isinstance(dataset, list):\n",
    "        dataset = list(dataset)\n",
    "\n",
    "    dataset.sort(key=lambda x: sort_function(x,weights))\n",
    "    if len(dataset) >= n:\n",
    "        return dataset[0:n-1]\n",
    "    return dataset\n",
    "\n",
    "def area_function(point):\n",
    "    area_covered = 1\n",
    "    for value in point:\n",
    "        area_covered = area_covered * (1-value)\n",
    "    return area_covered\n",
    "\n",
    "def get_angular_representatives2(dataset, n, weights):\n",
    "    if not isinstance(dataset, list):\n",
    "        dataset = list(dataset)\n",
    "\n",
    "    dataset.sort(key=lambda x: area_function(x), reverse = True)\n",
    "    if len(dataset) >= n:\n",
    "        return dataset[0:n-1]\n",
    "    return dataset\n",
    "\n",
    "\n",
    "\n",
    "def execute_get_angular_representatives(input_list, numRepr, weights):\n",
    "    i = 0\n",
    "    nd = []\n",
    "    for el_list in input_list:\n",
    "        nd.append(el_list[1])\n",
    "    nd = get_angular_representatives(nd, numRepr, weights)\n",
    "    return nd\n",
    "\n",
    "def execute_get_angular_representatives2(input_list, numRepr, weights):\n",
    "    i = 0\n",
    "    nd = []\n",
    "    for el_list in input_list:\n",
    "        nd.append(el_list[1])\n",
    "    nd = get_angular_representatives2(nd, numRepr, weights)\n",
    "    return nd\n",
    "def execute_filter_with_memory(input_list, repr, weights, onlyFilter):\n",
    "    i = 0\n",
    "    nd = []\n",
    "    for el_list in input_list:\n",
    "        nd.append(el_list[1])\n",
    "    nd = filter_with_memory(nd, repr, weights, onlyFilter)\n",
    "    return nd\n",
    "\n",
    "def representative_smallest(datapoints, weights, numRepr, numSlices = 4, onlyFilter = True):\n",
    "\n",
    "    dimensions = len(datapoints[0])\n",
    "    numRepr = floor(numRepr/(numSlices**(dimensions-1)))\n",
    "\n",
    "    rep = spark.sparkContext.parallelize(datapoints) \\\n",
    "                        .map(lambda x : ( get_angular_partitionIndex(x, dimensions, numSlices)  , x) )  \\\n",
    "                        .partitionBy(numSlices**(dimensions-1)) \\\n",
    "                        .mapPartitions(lambda x : execute_get_angular_representatives(x, numRepr, weights)) \\\n",
    "                        .collect()\n",
    "    start_parallel = time.time()\n",
    "    print(len(rep))\n",
    "    rep = find_skyline_sfs(rep, weights)\n",
    "    end = time.time()-start\n",
    "    print('Length of representatives: ' + str(len(rep)) + ', time taken to find: '+ str(end))\n",
    "    parallel_skyline = spark.sparkContext.parallelize(datapoints, 12)\\\n",
    "                                .mapPartitions(lambda x : filter_with_memory(x, rep, weights, onlyFilter)) \\\n",
    "                                .collect()\n",
    "    end_parallel = time.time() - start_parallel\n",
    "    \n",
    "    print('Time taken to filter: ' +str(end_parallel))\n",
    "    print('Length of the data after filter: ' + str(len(parallel_skyline)))\n",
    "    return parallel_skyline\n",
    "\n",
    "def representative_smallest_nd(datapoints, vertices, p, numRepr, numSlices = 2):\n",
    "\n",
    "    dimensions = len(datapoints[0])\n",
    "    numRepr = floor(numRepr/(numSlices**(dimensions-1)))\n",
    "    weights = compute_centroid(vertices)\n",
    "    representatives = spark.sparkContext.parallelize(datapoints) \\\n",
    "                        .map(lambda x : ( get_angular_partitionIndex(x, dimensions, numSlices)  , x) )  \\\n",
    "                        .partitionBy(numSlices**(dimensions-1)) \\\n",
    "                        .mapPartitions(lambda x : execute_get_angular_representatives(x, numRepr, weights)) \\\n",
    "                        .collect()\n",
    "    start_parallel = time.time()\n",
    "    print(len(representatives))\n",
    "    representatives = sve1f(representatives, vertices, p)\n",
    "    end = time.time()-start\n",
    "    print('Length of representatives: ' + str(len(representatives)) + ', time taken to find: '+ str(end))\n",
    "    parallel_skyline = spark.sparkContext.parallelize(datapoints, 12)\\\n",
    "                                .mapPartitions(lambda x : filter_nd_with_memory2(x, representatives, vertices, p, onlyFilter=True)) \\\n",
    "                                .collect()\n",
    "    end_parallel = time.time() - start_parallel\n",
    "\n",
    "    print('Time taken to filter: ' +str(end_parallel))\n",
    "    print('Length of the data after filter: ' + str(len(parallel_skyline)))\n",
    "    return parallel_skyline\n",
    "\n",
    "\n",
    "def grid_repr(datapoints, numSlicesPerDimension, weights, numRep):\n",
    "\n",
    "    limit = 1 / (numSlicesPerDimension)\n",
    "    #print('iterating - limit: ' +str(limit))\n",
    "    dimensions = len(datapoints[0])\n",
    "    num_slices_in_space = numSlicesPerDimension**dimensions\n",
    "    containerList = []\n",
    "    # create N square containers with each container having the datapoints contained and an index\n",
    "    for i in range(num_slices_in_space):\n",
    "        worst = []\n",
    "        best =[]\n",
    "        for j in range(dimensions):\n",
    "            #inizializzazione worst tuple\n",
    "            index_w = floor( i / (numSlicesPerDimension**j) ) % numSlicesPerDimension\n",
    "            worst.insert(j, index_w * limit + limit)\n",
    "            #inizializzazione best tuple\n",
    "            index_b = floor( i / (numSlicesPerDimension**j) ) % numSlicesPerDimension\n",
    "            best.insert(j, index_b * limit)\n",
    "\n",
    "        containerList.insert(i, Container(worst, best, []))\n",
    "\n",
    "    for dp in datapoints:\n",
    "        index = 0\n",
    "        for i in range(len(dp)):\n",
    "            if dp[i] >= 1:\n",
    "                index = index + floor(0.99999 / limit) * (numSlicesPerDimension**i)\n",
    "            else:\n",
    "                index = index + floor(dp[i] / limit) * (numSlicesPerDimension**i)\n",
    "        (containerList[index].dataContained).append(dp)\n",
    "\n",
    "    print(\"Number of containers before filtering: \" + str(len(containerList)))\n",
    "\n",
    "    print(len(containerList))\n",
    "    repr = []\n",
    "    k = 0\n",
    "    while k < len(containerList):\n",
    "        #print(k)\n",
    "        #print(len(containerList[k].dataContained))\n",
    "        if len(containerList[k].dataContained)==0:\n",
    "            k += 1\n",
    "            continue\n",
    "        containerList[k].dataContained.sort(key=lambda x: sort_function(x,weights))\n",
    "        #print(containerList[k].dataContained[0:10])\n",
    "        r = find_skyline_sfs(containerList[k].dataContained[0:numRep], weights)\n",
    "        repr= repr + r\n",
    "        k =(floor(k/numSlicesPerDimension)+1)*numSlicesPerDimension\n",
    "    return repr\n",
    "\n",
    "def grid_repr2(datapoints, numSlicesPerDimension, weights, numRep):\n",
    "\n",
    "    limit = 1 / (numSlicesPerDimension)\n",
    "    #print('iterating - limit: ' +str(limit))\n",
    "    dimensions = len(datapoints[0])\n",
    "    num_slices_in_space = numSlicesPerDimension**dimensions\n",
    "    containerList = []\n",
    "    # create N square containers with each container having the datapoints contained and an index\n",
    "    for i in range(num_slices_in_space):\n",
    "        worst = []\n",
    "        best =[]\n",
    "        for j in range(dimensions):\n",
    "            #inizializzazione worst tuple\n",
    "            index_w = floor( i / (numSlicesPerDimension**j) ) % numSlicesPerDimension\n",
    "            worst.insert(j, index_w * limit + limit)\n",
    "            #inizializzazione best tuple\n",
    "            index_b = floor( i / (numSlicesPerDimension**j) ) % numSlicesPerDimension\n",
    "            best.insert(j, index_b * limit)\n",
    "\n",
    "        containerList.insert(i, Container(worst, best, []))\n",
    "\n",
    "    for dp in datapoints:\n",
    "        index = 0\n",
    "        for i in range(len(dp)):\n",
    "            if dp[i] >= 1:\n",
    "                index = index + floor(0.99999 / limit) * (numSlicesPerDimension**i)\n",
    "            else:\n",
    "                index = index + floor(dp[i] / limit) * (numSlicesPerDimension**i)\n",
    "        (containerList[index].dataContained).append(dp)\n",
    "\n",
    "    print(len(containerList))\n",
    "    repr = []\n",
    "    k = 0\n",
    "    i = 0\n",
    "    while k < len(containerList):\n",
    "        #print(k)\n",
    "        #print(i)\n",
    "        #print(len(containerList[k].dataContained))\n",
    "        if len(containerList[i].dataContained)==0:\n",
    "            if(i < (k+numSlicesPerDimension**2)-1):\n",
    "                i = i + numSlicesPerDimension + 1\n",
    "                continue\n",
    "            else:\n",
    "                k = k + numSlicesPerDimension**2\n",
    "                i = k\n",
    "                continue\n",
    "        containerList[i].dataContained.sort(key=lambda x: sort_function(x,weights))\n",
    "        #print(containerList[k].dataContained[0:10])\n",
    "        r = find_skyline_sfs(containerList[i].dataContained[0:numRep], weights)\n",
    "        repr= repr + r\n",
    "        k = k + numSlicesPerDimension**2\n",
    "        i = k\n",
    "    return repr\n",
    "\n",
    "def grid_repr3(datapoints, numSlicesPerDimension, weights, numRep):\n",
    "\n",
    "    limit = 1 / (numSlicesPerDimension)\n",
    "    #print('iterating - limit: ' +str(limit))\n",
    "    dimensions = len(datapoints[0])\n",
    "    num_slices_in_space = numSlicesPerDimension**dimensions\n",
    "    containerList = []\n",
    "    # create N square containers with each container having the datapoints contained and an index\n",
    "    for i in range(num_slices_in_space):\n",
    "        worst = []\n",
    "        best =[]\n",
    "        for j in range(dimensions):\n",
    "            #inizializzazione worst tuple\n",
    "            index_w = floor( i / (numSlicesPerDimension**j) ) % numSlicesPerDimension\n",
    "            worst.insert(j, index_w * limit + limit)\n",
    "            #inizializzazione best tuple\n",
    "            index_b = floor( i / (numSlicesPerDimension**j) ) % numSlicesPerDimension\n",
    "            best.insert(j, index_b * limit)\n",
    "\n",
    "        containerList.insert(i, Container(worst, best, []))\n",
    "\n",
    "    for dp in datapoints:\n",
    "        index = 0\n",
    "        for i in range(len(dp)):\n",
    "            if dp[i] >= 1:\n",
    "                index = index + floor(0.99999 / limit) * (numSlicesPerDimension**i)\n",
    "            else:\n",
    "                index = index + floor(dp[i] / limit) * (numSlicesPerDimension**i)\n",
    "        (containerList[index].dataContained).append(dp)\n",
    "\n",
    "    print(len(containerList))\n",
    "    repr = []\n",
    "    k = 0\n",
    "    j = 0\n",
    "    t = 0\n",
    "    while k < len(containerList)-1:\n",
    "        print(k)\n",
    "        j = k\n",
    "        t = k + numSlicesPerDimension\n",
    "        #print(i)\n",
    "        #print(len(containerList[k].dataContained))\n",
    "        while j < k+numSlicesPerDimension-1:\n",
    "            i = j\n",
    "            while(True):\n",
    "                if len(containerList[i].dataContained)==0:\n",
    "                    if(i < (k+numSlicesPerDimension**2)-1):\n",
    "                        i = i + numSlicesPerDimension + 1\n",
    "                        continue\n",
    "                    else:\n",
    "                        break\n",
    "                containerList[i].dataContained.sort(key=lambda x: sort_function(x,weights))\n",
    "                r = find_skyline_sfs(containerList[i].dataContained[0:numRep], weights)\n",
    "                repr= repr + r\n",
    "                break\n",
    "            j = j + 1\n",
    "        while t < k-1+3*numSlicesPerDimension:\n",
    "            i = t\n",
    "            while(True):\n",
    "                if len(containerList[i].dataContained)==0:\n",
    "                    if(i < (k+numSlicesPerDimension**2)-1):\n",
    "                        i = i + numSlicesPerDimension + 1\n",
    "                        continue\n",
    "                    else:\n",
    "                        break\n",
    "                containerList[i].dataContained.sort(key=lambda x: sort_function(x,weights))\n",
    "                r = find_skyline_sfs(containerList[i].dataContained[0:numRep], weights)\n",
    "                repr= repr + r\n",
    "                break\n",
    "            t = t+ numSlicesPerDimension\n",
    "        k = k + numSlicesPerDimension**2\n",
    "        #print(containerList[k].dataContained[0:10])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    return repr\n",
    "\n",
    "def grid_representatives(datapoints, weights, n=4, numRep=10):\n",
    "    start = time.time()\n",
    "    representatives = grid_repr(datapoints, n, weights, numRep)\n",
    "    end = time.time() - start\n",
    "    print('Length rep: '+ str(len(representatives))+ 'time taken: '+str(end))\n",
    "    #print(representatives)\n",
    "    parallel_skyline = spark.sparkContext.parallelize(datapoints, 12)\\\n",
    "                                .mapPartitions(lambda x : filter_with_memory(x, representatives, weights, onlyFilter=True)) \\\n",
    "                                .collect()\n",
    "    end_parallel = time.time() - start\n",
    "\n",
    "    print('Time taken to filter: ' +str(end_parallel))\n",
    "    print('Length of the data after filter: ' + str(len(parallel_skyline)))\n",
    "    return parallel_skyline\n",
    "\n",
    "def grid_representatives2(datapoints, weights, n=4, numRep=10):\n",
    "    start = time.time()\n",
    "    representatives = grid_repr2(datapoints, n, weights, numRep)\n",
    "    end = time.time() - start\n",
    "    print('Length rep: '+ str(len(representatives))+ 'time taken: '+str(end))\n",
    "    #print(representatives)\n",
    "    parallel_skyline = spark.sparkContext.parallelize(datapoints, 12)\\\n",
    "                                .mapPartitions(lambda x : filter_with_memory(x, representatives, weights, onlyFilter=True)) \\\n",
    "                                .collect()\n",
    "    end_parallel = time.time() - start\n",
    "\n",
    "    print('Time taken to filter: ' +str(end_parallel))\n",
    "    print('Length of the data after filter: ' + str(len(parallel_skyline)))\n",
    "    return parallel_skyline\n",
    "\n",
    "def grid_representatives3(datapoints, weights, n=4, numRep=10):\n",
    "    start = time.time()\n",
    "    representatives = grid_repr3(datapoints, n, weights, numRep)\n",
    "    end = time.time() - start\n",
    "    print('Length rep: '+ str(len(representatives))+ 'time taken: '+str(end))\n",
    "    #print(representatives)\n",
    "    parallel_skyline = spark.sparkContext.parallelize(datapoints, 12)\\\n",
    "                                .mapPartitions(lambda x : filter_with_memory(x, representatives, weights, onlyFilter=True)) \\\n",
    "                                .collect()\n",
    "    end_parallel = time.time() - start\n",
    "\n",
    "    print('Time taken to filter: ' +str(end_parallel))\n",
    "    print('Length of the data after filter: ' + str(len(parallel_skyline)))\n",
    "    return parallel_skyline\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
